func (lt *Loadtest) run_retries{{if .RetriesEnabled}}Enabled{{else}}Disabled{{end}}_maxTasks{{if .MaxTasksGTZero}}GTZero{{else}}NotGTZero{{end}}_metrics{{if .MetricsEnabled}}Enabled{{else}}Disabled{{end}}(ctx context.Context, {{if .RetriesEnabled}}shutdownErrResp{{else}}_{{end}} *error) error {

	cfgUpdateChan := lt.cfgUpdateChan
	defer close(cfgUpdateChan)

	lt.startTime = time.Now()

	{{if .MetricsEnabled}}
	{

		csvFile, err := os.Create(lt.csvData.outputFilename)
		if err != nil {
			return fmt.Errorf("failed to open output csv metrics file for writing: %w", err)
		}
		defer lt.writeOutputCsvFooterAndClose(csvFile)

		lt.csvData.writeErr = lt.writeOutputCsvConfigComment(csvFile)

		if lt.csvData.writeErr == nil {

			lt.csvData.writer = bufio.NewWriter(csvFile)

			lt.csvData.writeErr = lt.writeOutputCsvHeaders()
		}
	}
	{{end}}

	lt.logger.LogAttrs(ctx, slog.LevelInfo,
		"starting loadtest",
		slog.Any("config", lt.loadtestConfigAsJson()),
	)

	{{if .MetricsEnabled}}
	var wg sync.WaitGroup

	wg.Go(lt.resultsHandler)
	{{end}}

	numWorkers := lt.numWorkers
	numSpawnedWorkers := 0

	// numTasks is the total number of tasks
	// scheduled to run ( including retries )
	var numTasks int

	intervalID := time.Now()
	{{if .MaxTasksGTZero}}
	maxTasks := lt.maxTasks
	{{end}}

	const maxConfigChanges = 6

	interval := lt.interval
	numNewTasks := lt.numIntervalTasks
	ctxDone := ctx.Done()
	taskReader := lt.taskReader
	configChanges := make([]slog.Attr, 0, maxConfigChanges)
	meta := taskMeta{
		NumIntervalTasks: lt.numIntervalTasks,
	}
	var interTaskInterval time.Duration
	if meta.NumIntervalTasks > 0 {
		interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
	}

	taskBuf := make([]Doer, 0, lt.maxIntervalTasks)

	var enqueueTasks func()
	var updateEnqueueTasksStrategy func()
	{
		floodStrategy := func(){
			for _, task := range taskBuf {
				lt.taskChan <- taskWithMeta{task, intervalID, meta}
			}
		}

		staggerStrategy := func(){
			lt.taskChan <- taskWithMeta{taskBuf[0], intervalID, meta}

			for _, task := range taskBuf[1:] {
				time.Sleep(interTaskInterval)
				lt.taskChan <- taskWithMeta{task, time.Now(), meta}
			}
		}

		updateEnqueueTasksStrategy = func() {
			if interTaskInterval <= skipInterTaskSchedulingThreshold {
				enqueueTasks = floodStrategy
			} else {
				enqueueTasks = staggerStrategy
			}
		}
	}
	updateEnqueueTasksStrategy()

	var delay time.Duration

	{{if .RetriesEnabled}}
	readRetries := func(p []Doer) int {
		// make sure you only fill up to len

		var i int
		for i < len(p) {
			select {
			case task := <-lt.retryTaskChan:
				p[i] = task
			default:
				return i
			}
			i++
		}

		return i
	}
	{{- end}}

	// stopping routine runs on return
	// flushing as much as possible
	defer func() {

		{{if .RetriesEnabled}}
		err := func(flushRetries bool) error {
			if !flushRetries {

				lt.logger.LogAttrs(ctx, slog.LevelDebug,
					"not waiting on retries to flush on shutdown",
					slog.String("reason", "retries disabled or flush retries on shutdown disabled"),
					slog.Int("num_tasks", numTasks),
				)

				return nil
			}

			if err := ctx.Err(); err != nil {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"not waiting on retries to flush on shutdown",
					slog.String("reason", "user stopped loadtest"),
					slog.Int("num_tasks", numTasks),
					slog.Any("error", err),
				)
				return nil
			}

			lt.logger.LogAttrs(ctx, slog.LevelDebug,
				"waiting on retries to flush",
				slog.Int("num_tasks", numTasks),
			)

			if meta.NumIntervalTasks <= 0 || numWorkers <= 0 {

				lt.logger.LogAttrs(ctx, slog.LevelError,
					"retry flushing could not be attempted",
					slog.Int("num_tasks", numTasks),
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
				)

				return ErrRetriesFailedToFlush
			}

			preflushNumTasks := numTasks

			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"shutting down: flushing retries",
				slog.Int("num_tasks", numTasks),
				slog.String("flush_retries_timeout", lt.flushRetriesTimeout.String()),
			)

			shutdownCtx, cancel := context.WithTimeout(context.Background(), lt.flushRetriesTimeout)
			defer cancel()

			intervalID = time.Now()
			taskBuf = taskBuf[:0]
			{{if .MetricsEnabled}}meta.Lag = 0{{end}}

			for {

				if err := shutdownCtx.Err(); err != nil {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"failed to flush all retries",
						slog.Int("preflush_num_tasks", preflushNumTasks),
						slog.Int("num_tasks", numTasks),
						slog.Any("error", err),
					)

					return ErrRetriesFailedToFlush
				}

				lt.resultWaitGroup.Wait()

				for {

					if err := shutdownCtx.Err(); err != nil {
						lt.logger.LogAttrs(ctx, slog.LevelError,
							"failed to flush all retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
							slog.Any("error", err),
						)

						return ErrRetriesFailedToFlush
					}

					{{if .MaxTasksGTZero -}}
					if numTasks >= maxTasks {
						lt.logger.LogAttrs(ctx, slog.LevelError,
							"failed to flush all retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
							slog.String("reason", "reached max tasks"),
						)
						return ErrRetriesFailedToFlush
					}

					// 1. the below looks off/odd, why not use?:
					//
					// ```
					// if n := maxTasks - numTasks; n < numNewTasks {
					// 	numNewTasks = n
					// }
					// ```
					//
					// 2. And for that matter, why not keep meta.NumIntervalTasks in sync with numNewTasks?
					//
					// ---
					//
					// 1. The implementation would be exactly the same, just using another variable
					// 2. the meta.NumIntervalTasks value is used in RATE calculations, if we keep it in sync
					//    with BOUNDS values then the last tasks could run at a lower RATE than intended. It
					//    is only kept in sync when a user adjusts the RATE via a ConfigUpdate. Don't confuse
					//    bounds purpose values with rate purpose values.
					//
					numNewTasks = maxTasks - numTasks
					if numNewTasks > meta.NumIntervalTasks {
						numNewTasks = meta.NumIntervalTasks
					}
					{{end}}

					select {
					case <-ctxDone:
						lt.logger.LogAttrs(ctx, slog.LevelWarn,
							"user stopped loadtest while attempting to flush retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
						)
						return nil
					default:
						// continue with load generating retries
					}

					// acquire load generation opportunity slots ( smooths bursts )
					//
					// in the shutdown retry flow we always want to acquire before reading retries
					// to avoid a deadlock edge case of the retry queue being full, all workers tasks failed and need to be retried
					if err := lt.intervalTasksSema.Acquire(shutdownCtx, int64(numNewTasks)); err != nil {
						lt.logger.LogAttrs(ctx, slog.LevelError,
							"failed to flush all retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
							slog.Any("error", err),
							slog.String("reason", "shutdown timeout likely reached while waiting for semaphore acquisition"),
						)
						return ErrRetriesFailedToFlush
					}

					// read up to numNewTasks from retry slice
					taskBufSize := readRetries(taskBuf[:numNewTasks:numNewTasks])
					if taskBufSize <= 0 {
						// wait for any pending tasks to flush and try read again

						lt.logger.LogAttrs(ctx, slog.LevelDebug,
							"verifying all retries have flushed",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
						)

						lt.resultWaitGroup.Wait()

						// read up to numNewTasks from retry slice again
						taskBufSize = readRetries(taskBuf[:numNewTasks:numNewTasks])
						if taskBufSize <= 0 {

							lt.logger.LogAttrs(ctx, slog.LevelInfo,
								"all retries flushed",
								slog.Int("preflush_num_tasks", preflushNumTasks),
								slog.Int("num_tasks", numTasks),
							)
							return nil
						}
					}
					taskBuf = taskBuf[:taskBufSize]

					// re-release any extra load slots we allocated beyond what really remains to do
					if numNewTasks > taskBufSize {
						lt.intervalTasksSema.Release(int64(numNewTasks - taskBufSize))
					}

					lt.resultWaitGroup.Add(taskBufSize+1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
					lt.resultsChan <- taskResult{
						Meta: taskMeta{
							// IntervalID: intervalID, // not required unless in a debug context
							SampleSize: taskBufSize,
						},
					}

					meta.IntervalID = intervalID

					enqueueTasks()

					taskBuf = taskBuf[:0]

					numTasks += taskBufSize

					{{if .MetricsEnabled}}meta.Lag = 0{{end}}

					// wait for next interval time to exist
					nextIntervalID := intervalID.Add(interval)
					realNow := time.Now()
					delay = nextIntervalID.Sub(realNow)
					if delay > 0 {
						time.Sleep(delay)
						intervalID = nextIntervalID

						if taskBufSize < numNewTasks {
							// just finished this iteration of retry enqueuing
							//
							// break to loop through retry drain context again
							break
						}

						continue
					}

					if delay < 0 {
						intervalID = realNow

						{{if .MetricsEnabled}}
						lag := -delay
						meta.Lag = lag

						lt.resultWaitGroup.Add(1)
						lt.resultsChan <- taskResult{
							Meta: taskMeta{
								// IntervalID: intervalID, // not required unless in a debug context
								Lag:        lag,
							},
						}
						{{end}}
					}

					if taskBufSize < numNewTasks {
						// just finished this iteration of retry enqueuing
						//
						// break to loop through retry drain context again
						break
					}
				}
			}
		}(lt.flushRetriesOnShutdown)
		if err != nil {
			*shutdownErrResp = err
		}
		{{- end}}

		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for running tasks to stop",
		)
		lt.resultWaitGroup.Wait()

		{{if .MetricsEnabled}}
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping result handler routine",
		)

		// signal for result handler routines to stop
		close(lt.resultsChan)
		{{end}}

		// signal for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping workers",
		)
		for i := 0; i < numSpawnedWorkers; i++ {
			close(lt.pauseChans[i])
		}

		{{if .MetricsEnabled}}
		// wait for result handler routines to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for result handler routines to stop",
		)
		wg.Wait()
		{{end}}

		// wait for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for workers to stop",
		)
		lt.workerWaitGroup.Wait()

		lt.logger.LogAttrs(ctx, slog.LevelInfo,
			"loadtest stopped",
		)
	}()

	// getTaskSlotCount is the task emission back pressure
	// throttle that conveys the number of tasks that
	// are allowed to be un-finished for the performance
	// interval under normal circumstances
	getTaskSlotCount := func() int {
		return maxPendingTasks(numWorkers, numNewTasks)
	}

	// apply initial task buffer limits to the interval semaphore
	taskSlotCount := getTaskSlotCount()
	lt.intervalTasksSema.Release(int64(taskSlotCount))

	configCausesPause := func() bool {
		return meta.NumIntervalTasks <= 0 || numWorkers <= 0
	}

	var paused bool
	var pauseStart time.Time

	handleConfigUpdateAndPauseState := func(cu ConfigUpdate) error {
		for {
			var prepSemaErr error
			var recomputeInterTaskInterval, recomputeTaskSlots bool

			if cu.numWorkers.set {
				recomputeTaskSlots = true

				n := cu.numWorkers.val

				// prevent over committing on the maxWorkers count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxWorkers {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too high"),
						slog.String("remediation_hint", "increase the loadtest MaxWorkers setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxWorkers),
					)
					n = lt.maxWorkers
				}

				if n > numWorkers {

					// unpause workers
					for i := numWorkers; i < numSpawnedWorkers; i++ {
						lt.pauseChans[i] <- struct{}{}
					}

					// spawn new workers if needed
					for i := numSpawnedWorkers; i < n; i++ {
						lt.addWorker(ctx, i)
						numSpawnedWorkers++
					}
				} else if n < numWorkers {

					// pause workers if needed
					for i := numWorkers - 1; i >= n; i-- {
						lt.pauseChans[i] <- struct{}{}
					}
				}

				configChanges = append(configChanges,
					slog.Int("old_num_workers", numWorkers),
					slog.Int("new_num_workers", n),
				)
				numWorkers = n
			}

			if cu.numIntervalTasks.set {
				recomputeInterTaskInterval = true
				recomputeTaskSlots = true

				n := cu.numIntervalTasks.val

				// prevent over committing on the maxIntervalTasks count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxIntervalTasks {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too high"),
						slog.String("remediation_hint", "increase the loadtest MaxIntervalTasks setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxIntervalTasks),
					)
					n = lt.maxIntervalTasks
				}

				configChanges = append(configChanges,
					slog.Int("old_num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("new_num_interval_tasks", n),
				)
				numNewTasks = n
				meta.NumIntervalTasks = n
			}

			if cu.interval.set {
				recomputeInterTaskInterval = true

				n := cu.interval.val

				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: interval",
						slog.String("reason", "update tried to set interval too low"),
						slog.String("remediation_taken", "using min value"),
						slog.String("requested", n.String()),
						slog.String("min", time.Duration(0).String()),
					)
					n = 0
				}

				configChanges = append(configChanges,
					slog.String("old_interval", interval.String()),
					slog.String("new_interval", n.String()),
				)
				interval = n
			}

			// && clause: protects against divide by zero
			if recomputeInterTaskInterval && meta.NumIntervalTasks > 0 {
				interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
				updateEnqueueTasksStrategy()
			}

			if recomputeTaskSlots {
				if newTaskSlotCount := getTaskSlotCount(); newTaskSlotCount != taskSlotCount {

					if newTaskSlotCount > taskSlotCount {
						lt.intervalTasksSema.Release(int64(newTaskSlotCount - taskSlotCount))
					} else {
						prepSemaErr = lt.intervalTasksSema.Acquire(ctx, int64(taskSlotCount-newTaskSlotCount))
						if prepSemaErr != nil {
							lt.logger.LogAttrs(ctx, slog.LevelError,
								"loadtest config update: failed to pre-acquire load generation slots",
								slog.Any("error", prepSemaErr),
							)

							// not returning and error... yet
							// going to let config update log statement occur and then report the error present in prepSemaErr
						}
					}

					taskSlotCount = newTaskSlotCount
				}
			}

			if !cu.onStartup {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"loadtest config updated",
					configChanges...,
				)
			}
			configChanges = configChanges[:0]

			if prepSemaErr != nil {
				return prepSemaErr
			}

			// pause load generation if unable to schedule anything
			if configCausesPause() {

				if !paused {
					paused = true
					pauseStart = time.Now().UTC()

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"pausing load generation",
						slog.Int("num_interval_tasks", meta.NumIntervalTasks),
						slog.Int("num_workers", numWorkers),
						slog.String("paused_at", pauseStart.String()),
					)
				}

				// duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
				// that follows
				//
				// ref: https://go.dev/ref/spec#Select_statements
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				default:
				}
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				case cu = <-cfgUpdateChan:
					continue
				}
			}

			if paused {
				paused = false
				intervalID = time.Now()

				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"resuming load generation",
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
					slog.String("paused_at", pauseStart.String()),
					slog.String("resumed_at", intervalID.UTC().String()),
				)
			}

			return nil
		}
	}

	if configCausesPause() {
		if err := handleConfigUpdateAndPauseState(ConfigUpdate{onStartup: true}); err != nil {
			if err == errLoadtestContextDone {
				return nil
			}
			return err
		}
	}

	// start workers just before starting task scheduling
	for i := 0; i < numWorkers; i++ {
		lt.addWorker(ctx, i)
		numSpawnedWorkers++
	}

	// main task scheduling loop
	for {
		{{if .MaxTasksGTZero -}}
		if numTasks >= maxTasks {
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"loadtest finished: max task count reached",
				slog.Int("max_tasks", maxTasks),
			)
			return nil
		}

		numNewTasks = maxTasks - numTasks
		if numNewTasks > meta.NumIntervalTasks {
			numNewTasks = meta.NumIntervalTasks
		}
		{{end}}

		// Not duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
		// that follows because this is a ordered sequence where all selects in the sequence have non-blocking default cases.
		//
		// The odds of thread switching between these selects is minimal as is the impact of having to wait at most one more cycle
		// to short circuit and return.
		//
		// ref: https://go.dev/ref/spec#Select_statements
		select {
		case <-ctxDone:
			return nil
		default:
		}
		select {
		case cu := <-cfgUpdateChan:
			if err := handleConfigUpdateAndPauseState(cu); err != nil {
				if err == errLoadtestContextDone {
					return nil
				}
				return err
			}

			// re-loop
			continue
		default:
			// continue with load generation
		}

		taskBufSize := 0
		{{- if .RetriesEnabled}}

		//
		// read up to numNewTasks from retry slice
		//

		// acquire load generation opportunity slots ( smooths bursts )
		//
		// do this early conditionally to allow retries to settle in the retry channel
		// so we can pick them up when enough buffer space has cleared
		//
		// thus we avoid a possible deadlock where the retry queue is full and the workers
		// all have failed tasks that wish to be retried
		if lt.intervalTasksSema.Acquire(ctx, int64(numNewTasks)) != nil {
			return nil
		}

		taskBufSize = readRetries(taskBuf[:numNewTasks:numNewTasks])

		{{end}}
		taskBuf = taskBuf[:taskBufSize]

		if taskBufSize < numNewTasks {
			maxSize := numNewTasks - taskBufSize
			n := taskReader.ReadTasks(taskBuf[taskBufSize:numNewTasks:numNewTasks])
			if n < 0 || n > maxSize {
				panic(ErrBadReadTasksImpl)
			}
			if n == 0 {
				{{if .RetriesEnabled}}
				// iteration is technically done now
				// but there could be straggling retries
				// queued after this, those should continue
				// to be flushed if and only if maxTasks
				// has not been reached and if it is greater
				// than zero
				if taskBufSize == 0 {
					// return immediately if there is nothing
					// new to enqueue

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"stopping loadtest: ReadTasks did not load enough tasks",
						slog.Int("final_task_delta", 0),
					)

					return nil
				}

				lt.logger.LogAttrs(ctx, slog.LevelDebug,
					"scheduled: stopping loadtest: ReadTasks did not load enough tasks",
					slog.Int("retry_count", taskBufSize),
				)
				{{- else}}
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"stopping loadtest: ReadTasks did not load enough tasks",
					slog.Int("final_task_delta", 0),
				)

				return nil
				{{- end}}
			}

			taskBufSize += n
			taskBuf = taskBuf[:taskBufSize]
		}

		// acquire load generation opportunity slots ( smooths bursts )
		// if not done already{{if .RetriesEnabled}}
		//
		// but if we allocated too many in our retry prep phase then release the
		// difference
		if numNewTasks > taskBufSize {
			lt.intervalTasksSema.Release(int64(numNewTasks - taskBufSize))
		}
		{{else}}
		if lt.intervalTasksSema.Acquire(ctx, int64(taskBufSize)) != nil {
			return nil
		}
		{{end}}

		lt.resultWaitGroup.Add(taskBufSize+1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
		lt.resultsChan <- taskResult{
			Meta: taskMeta{
				// IntervalID: intervalID, // not required unless in a debug context
				SampleSize: taskBufSize,
			},
		}

		meta.IntervalID = intervalID

		enqueueTasks()

		if numNewTasks > taskBufSize {
			// must have hit the end of ReadTasks iterator
			// increase numTasks total by actual number queued
			// and stop traffic generation
			{{if not .RetriesEnabled}}// {{end}}numTasks += taskBufSize{{if not .RetriesEnabled}} // this line only has an effect except in a retries enabled context{{end}}
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"stopping loadtest: ReadTasks did not load enough tasks",
				slog.Int("final_task_delta", taskBufSize),
			)
			return nil
		}

		taskBuf = taskBuf[:0]

		numTasks += taskBufSize

		{{if .MetricsEnabled}}meta.Lag = 0{{end}}

		// wait for next interval time to exist
		nextIntervalID := intervalID.Add(interval)
		realNow := time.Now()
		delay = nextIntervalID.Sub(realNow)
		if delay > 0 {
			time.Sleep(delay)
			intervalID = nextIntervalID
			continue
		}

		if delay < 0 {
			intervalID = realNow

			{{if .MetricsEnabled}}
			lag := -delay
			meta.Lag = lag

			lt.resultWaitGroup.Add(1)
			lt.resultsChan <- taskResult{
				Meta: taskMeta{
					// IntervalID: intervalID, // not required unless in a debug context
					Lag:        lag,
				},
			}
			{{end}}
		}
	}
}
