// Code generated by ./internal/cmd/generate/main.go DO NOT EDIT.

package loadtester

import (
	"context"
	"encoding/csv"
	"fmt"
	"log/slog"
	"math"
	"math/big"
	"os"
	"strconv"
	"sync"
	"time"
)

type metricRecordResetables_retryEnabled_varianceEnabled struct {
	numTasks                           int
	numPass                            int
	numFail                            int
	numRetry                           int
	numPanic                           int
	sumLag                             time.Duration
	lag                                time.Duration
	minTaskDuration, maxTaskDuration   time.Duration
	minQueueDuration, maxQueueDuration time.Duration

	welfords struct {
		queue welfordVariance
		task  welfordVariance
	}
}

type metricRecord_retryEnabled_maxTasksGTZero_percentileEnabled_varianceEnabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	totalNumTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryEnabled_varianceEnabled

	latencies latencyLists
}

func (mr *metricRecord_retryEnabled_maxTasksGTZero_percentileEnabled_varianceEnabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryEnabled_varianceEnabled = metricRecordResetables_retryEnabled_varianceEnabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

	mr.latencies.queue.reset()
	mr.latencies.task.reset()

}

type metricRecordResetables_retryEnabled_varianceDisabled struct {
	numTasks                           int
	numPass                            int
	numFail                            int
	numRetry                           int
	numPanic                           int
	sumLag                             time.Duration
	lag                                time.Duration
	minTaskDuration, maxTaskDuration   time.Duration
	minQueueDuration, maxQueueDuration time.Duration
}

type metricRecord_retryEnabled_maxTasksGTZero_percentileEnabled_varianceDisabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	totalNumTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryEnabled_varianceDisabled

	latencies latencyLists
}

func (mr *metricRecord_retryEnabled_maxTasksGTZero_percentileEnabled_varianceDisabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryEnabled_varianceDisabled = metricRecordResetables_retryEnabled_varianceDisabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

	mr.latencies.queue.reset()
	mr.latencies.task.reset()

}

type metricRecord_retryEnabled_maxTasksGTZero_percentileDisabled_varianceEnabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	totalNumTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryEnabled_varianceEnabled
}

func (mr *metricRecord_retryEnabled_maxTasksGTZero_percentileDisabled_varianceEnabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryEnabled_varianceEnabled = metricRecordResetables_retryEnabled_varianceEnabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

}

type metricRecord_retryEnabled_maxTasksGTZero_percentileDisabled_varianceDisabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	totalNumTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryEnabled_varianceDisabled
}

func (mr *metricRecord_retryEnabled_maxTasksGTZero_percentileDisabled_varianceDisabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryEnabled_varianceDisabled = metricRecordResetables_retryEnabled_varianceDisabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

}

type metricRecord_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryEnabled_varianceEnabled

	latencies latencyLists
}

func (mr *metricRecord_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryEnabled_varianceEnabled = metricRecordResetables_retryEnabled_varianceEnabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

	mr.latencies.queue.reset()
	mr.latencies.task.reset()

}

type metricRecord_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryEnabled_varianceDisabled

	latencies latencyLists
}

func (mr *metricRecord_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryEnabled_varianceDisabled = metricRecordResetables_retryEnabled_varianceDisabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

	mr.latencies.queue.reset()
	mr.latencies.task.reset()

}

type metricRecord_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryEnabled_varianceEnabled
}

func (mr *metricRecord_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryEnabled_varianceEnabled = metricRecordResetables_retryEnabled_varianceEnabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

}

type metricRecord_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryEnabled_varianceDisabled
}

func (mr *metricRecord_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryEnabled_varianceDisabled = metricRecordResetables_retryEnabled_varianceDisabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

}

type metricRecordResetables_retryDisabled_varianceEnabled struct {
	numTasks                           int
	numPass                            int
	numFail                            int
	numPanic                           int
	sumLag                             time.Duration
	lag                                time.Duration
	minTaskDuration, maxTaskDuration   time.Duration
	minQueueDuration, maxQueueDuration time.Duration

	welfords struct {
		queue welfordVariance
		task  welfordVariance
	}
}

type metricRecord_retryDisabled_maxTasksGTZero_percentileEnabled_varianceEnabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	totalNumTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryDisabled_varianceEnabled

	latencies latencyLists
}

func (mr *metricRecord_retryDisabled_maxTasksGTZero_percentileEnabled_varianceEnabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryDisabled_varianceEnabled = metricRecordResetables_retryDisabled_varianceEnabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

	mr.latencies.queue.reset()
	mr.latencies.task.reset()

}

type metricRecordResetables_retryDisabled_varianceDisabled struct {
	numTasks                           int
	numPass                            int
	numFail                            int
	numPanic                           int
	sumLag                             time.Duration
	lag                                time.Duration
	minTaskDuration, maxTaskDuration   time.Duration
	minQueueDuration, maxQueueDuration time.Duration
}

type metricRecord_retryDisabled_maxTasksGTZero_percentileEnabled_varianceDisabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	totalNumTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryDisabled_varianceDisabled

	latencies latencyLists
}

func (mr *metricRecord_retryDisabled_maxTasksGTZero_percentileEnabled_varianceDisabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryDisabled_varianceDisabled = metricRecordResetables_retryDisabled_varianceDisabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

	mr.latencies.queue.reset()
	mr.latencies.task.reset()

}

type metricRecord_retryDisabled_maxTasksGTZero_percentileDisabled_varianceEnabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	totalNumTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryDisabled_varianceEnabled
}

func (mr *metricRecord_retryDisabled_maxTasksGTZero_percentileDisabled_varianceEnabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryDisabled_varianceEnabled = metricRecordResetables_retryDisabled_varianceEnabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

}

type metricRecord_retryDisabled_maxTasksGTZero_percentileDisabled_varianceDisabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	totalNumTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryDisabled_varianceDisabled
}

func (mr *metricRecord_retryDisabled_maxTasksGTZero_percentileDisabled_varianceDisabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryDisabled_varianceDisabled = metricRecordResetables_retryDisabled_varianceDisabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

}

type metricRecord_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryDisabled_varianceEnabled

	latencies latencyLists
}

func (mr *metricRecord_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryDisabled_varianceEnabled = metricRecordResetables_retryDisabled_varianceEnabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

	mr.latencies.queue.reset()
	mr.latencies.task.reset()

}

type metricRecord_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryDisabled_varianceDisabled

	latencies latencyLists
}

func (mr *metricRecord_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryDisabled_varianceDisabled = metricRecordResetables_retryDisabled_varianceDisabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

	mr.latencies.queue.reset()
	mr.latencies.task.reset()

}

type metricRecord_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryDisabled_varianceEnabled
}

func (mr *metricRecord_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryDisabled_varianceEnabled = metricRecordResetables_retryDisabled_varianceEnabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

}

type metricRecord_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled struct {
	// fields that are preserved
	intervalID       time.Time
	numIntervalTasks int

	sumTaskDuration, sumQueueDuration big.Int

	metricRecordResetables_retryDisabled_varianceDisabled
}

func (mr *metricRecord_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled) reset() {
	mr.sumTaskDuration.SetUint64(0)
	mr.sumQueueDuration.SetUint64(0)
	mr.metricRecordResetables_retryDisabled_varianceDisabled = metricRecordResetables_retryDisabled_varianceDisabled{
		minTaskDuration:  math.MaxInt64,
		minQueueDuration: math.MaxInt64,
	}

}

func (lt *Loadtest) doTask_retriesEnabled_metricsEnabled_taskMetadataProviderEnabled(ctx context.Context, workerID int, twm taskWithMeta) {
	taskStart := time.Now()

	var respFlags taskResultFlags
	{
		defer func() {
			taskEnd := time.Now()

			lt.resultsChan <- taskResult{
				taskResultFlags: respFlags,
				QueueDuration:   taskStart.Sub(twm.enqueueTime),
				TaskDuration:    taskEnd.Sub(taskStart),
				Meta:            twm.meta,
			}
		}()
	}

	tm := newTaskMetadata()
	defer releaseTaskMetadata(tm)

	tm.enqueueTime = twm.enqueueTime
	tm.dequeueTime = taskStart
	tm.meta = twm.meta
	ctx = injectTaskMetadataProvider(ctx, tm)

	// phase is the name of the step which has possibly caused a panic
	phase := taskPhaseDo

	var rt *retryTask
	if v, ok := twm.doer.(*retryTask); ok {
		rt = v
		phase = taskPhaseRetry
		defer func() {
			*rt = retryTask{}
			lt.retryTaskPool.Put(v)
		}()
	}
	defer func() {

		if r := recover(); r != nil {

			respFlags.Panicked = 1
			respFlags.Errored = 1

			const evtName = "worker recovered from panic"

			switch v := r.(type) {
			case error:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.Any("error", v),
				)
			case []byte:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", string(v)),
				)
			case string:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", v),
				)
			default:
				const msg = "unknown cause"

				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", msg),
				)
			}
		}
	}()
	err := twm.doer.Do(ctx, workerID)
	phase = taskPhaseInvalid // done, no panic occurred
	if err == nil {
		respFlags.Passed = 1
		return
	}

	lt.logger.LogAttrs(ctx, slog.LevelWarn,
		"task error",
		slog.Int("worker_id", workerID),
		slog.Any("error", err),
	)

	respFlags.Errored = 1

	var dr DoRetryer
	if rt != nil {
		dr = rt.DoRetryer
	} else if v, ok := twm.doer.(DoRetryer); ok {
		dr = v
	} else {
		return
	}

	phase = taskPhaseCanRetry
	if v, ok := dr.(DoRetryChecker); ok && !v.CanRetry(ctx, workerID, err) {
		phase = taskPhaseInvalid // done, no panic occurred
		return
	}
	phase = taskPhaseInvalid // done, no panic occurred

	// queue a new retry task
	{
		rt := lt.retryTaskPool.Get().(*retryTask)

		*rt = retryTask{dr, err}

		lt.retryTaskChan <- rt
	}

	respFlags.RetryQueued = 1
	return
}

func (lt *Loadtest) doTask_retriesEnabled_metricsEnabled_taskMetadataProviderDisabled(ctx context.Context, workerID int, twm taskWithMeta) {

	var respFlags taskResultFlags
	{
		taskStart := time.Now()
		defer func() {
			taskEnd := time.Now()

			lt.resultsChan <- taskResult{
				taskResultFlags: respFlags,
				QueueDuration:   taskStart.Sub(twm.enqueueTime),
				TaskDuration:    taskEnd.Sub(taskStart),
				Meta:            twm.meta,
			}
		}()
	}

	// phase is the name of the step which has possibly caused a panic
	phase := taskPhaseDo

	var rt *retryTask
	if v, ok := twm.doer.(*retryTask); ok {
		rt = v
		phase = taskPhaseRetry
		defer func() {
			*rt = retryTask{}
			lt.retryTaskPool.Put(v)
		}()
	}
	defer func() {

		if r := recover(); r != nil {

			respFlags.Panicked = 1
			respFlags.Errored = 1

			const evtName = "worker recovered from panic"

			switch v := r.(type) {
			case error:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.Any("error", v),
				)
			case []byte:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", string(v)),
				)
			case string:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", v),
				)
			default:
				const msg = "unknown cause"

				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", msg),
				)
			}
		}
	}()
	err := twm.doer.Do(ctx, workerID)
	phase = taskPhaseInvalid // done, no panic occurred
	if err == nil {
		respFlags.Passed = 1
		return
	}

	lt.logger.LogAttrs(ctx, slog.LevelWarn,
		"task error",
		slog.Int("worker_id", workerID),
		slog.Any("error", err),
	)

	respFlags.Errored = 1

	var dr DoRetryer
	if rt != nil {
		dr = rt.DoRetryer
	} else if v, ok := twm.doer.(DoRetryer); ok {
		dr = v
	} else {
		return
	}

	phase = taskPhaseCanRetry
	if v, ok := dr.(DoRetryChecker); ok && !v.CanRetry(ctx, workerID, err) {
		phase = taskPhaseInvalid // done, no panic occurred
		return
	}
	phase = taskPhaseInvalid // done, no panic occurred

	// queue a new retry task
	{
		rt := lt.retryTaskPool.Get().(*retryTask)

		*rt = retryTask{dr, err}

		lt.retryTaskChan <- rt
	}

	respFlags.RetryQueued = 1
	return
}

func (lt *Loadtest) doTask_retriesEnabled_metricsDisabled_taskMetadataProviderEnabled(ctx context.Context, workerID int, twm taskWithMeta) {
	taskStart := time.Now()

	defer lt.resultWaitGroup.Done()

	tm := newTaskMetadata()
	defer releaseTaskMetadata(tm)

	tm.enqueueTime = twm.enqueueTime
	tm.dequeueTime = taskStart
	tm.meta = twm.meta
	ctx = injectTaskMetadataProvider(ctx, tm)

	// phase is the name of the step which has possibly caused a panic
	phase := taskPhaseDo

	var rt *retryTask
	if v, ok := twm.doer.(*retryTask); ok {
		rt = v
		phase = taskPhaseRetry
		defer func() {
			*rt = retryTask{}
			lt.retryTaskPool.Put(v)
		}()
	}
	defer func() {

		if r := recover(); r != nil {

			const evtName = "worker recovered from panic"

			switch v := r.(type) {
			case error:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.Any("error", v),
				)
			case []byte:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", string(v)),
				)
			case string:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", v),
				)
			default:
				const msg = "unknown cause"

				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", msg),
				)
			}
		}
	}()
	err := twm.doer.Do(ctx, workerID)
	phase = taskPhaseInvalid // done, no panic occurred
	if err == nil {

		return
	}

	lt.logger.LogAttrs(ctx, slog.LevelWarn,
		"task error",
		slog.Int("worker_id", workerID),
		slog.Any("error", err),
	)

	var dr DoRetryer
	if rt != nil {
		dr = rt.DoRetryer
	} else if v, ok := twm.doer.(DoRetryer); ok {
		dr = v
	} else {
		return
	}

	phase = taskPhaseCanRetry
	if v, ok := dr.(DoRetryChecker); ok && !v.CanRetry(ctx, workerID, err) {
		phase = taskPhaseInvalid // done, no panic occurred
		return
	}
	phase = taskPhaseInvalid // done, no panic occurred

	// queue a new retry task
	{
		rt := lt.retryTaskPool.Get().(*retryTask)

		*rt = retryTask{dr, err}

		lt.retryTaskChan <- rt
	}

	return
}

func (lt *Loadtest) doTask_retriesEnabled_metricsDisabled_taskMetadataProviderDisabled(ctx context.Context, workerID int, twm taskWithMeta) {

	defer lt.resultWaitGroup.Done()

	// phase is the name of the step which has possibly caused a panic
	phase := taskPhaseDo

	var rt *retryTask
	if v, ok := twm.doer.(*retryTask); ok {
		rt = v
		phase = taskPhaseRetry
		defer func() {
			*rt = retryTask{}
			lt.retryTaskPool.Put(v)
		}()
	}
	defer func() {

		if r := recover(); r != nil {

			const evtName = "worker recovered from panic"

			switch v := r.(type) {
			case error:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.Any("error", v),
				)
			case []byte:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", string(v)),
				)
			case string:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", v),
				)
			default:
				const msg = "unknown cause"

				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", msg),
				)
			}
		}
	}()
	err := twm.doer.Do(ctx, workerID)
	phase = taskPhaseInvalid // done, no panic occurred
	if err == nil {

		return
	}

	lt.logger.LogAttrs(ctx, slog.LevelWarn,
		"task error",
		slog.Int("worker_id", workerID),
		slog.Any("error", err),
	)

	var dr DoRetryer
	if rt != nil {
		dr = rt.DoRetryer
	} else if v, ok := twm.doer.(DoRetryer); ok {
		dr = v
	} else {
		return
	}

	phase = taskPhaseCanRetry
	if v, ok := dr.(DoRetryChecker); ok && !v.CanRetry(ctx, workerID, err) {
		phase = taskPhaseInvalid // done, no panic occurred
		return
	}
	phase = taskPhaseInvalid // done, no panic occurred

	// queue a new retry task
	{
		rt := lt.retryTaskPool.Get().(*retryTask)

		*rt = retryTask{dr, err}

		lt.retryTaskChan <- rt
	}

	return
}

func (lt *Loadtest) doTask_retriesDisabled_metricsEnabled_taskMetadataProviderEnabled(ctx context.Context, workerID int, twm taskWithMeta) {
	taskStart := time.Now()

	var respFlags taskResultFlags
	{
		defer func() {
			taskEnd := time.Now()

			lt.resultsChan <- taskResult{
				taskResultFlags: respFlags,
				QueueDuration:   taskStart.Sub(twm.enqueueTime),
				TaskDuration:    taskEnd.Sub(taskStart),
				Meta:            twm.meta,
			}
		}()
	}

	tm := newTaskMetadata()
	defer releaseTaskMetadata(tm)

	tm.enqueueTime = twm.enqueueTime
	tm.dequeueTime = taskStart
	tm.meta = twm.meta
	ctx = injectTaskMetadataProvider(ctx, tm)

	// phase is the name of the step which has possibly caused a panic
	phase := taskPhaseDo

	defer func() {

		if r := recover(); r != nil {

			respFlags.Panicked = 1
			respFlags.Errored = 1

			const evtName = "worker recovered from panic"

			switch v := r.(type) {
			case error:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.Any("error", v),
				)
			case []byte:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", string(v)),
				)
			case string:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", v),
				)
			default:
				const msg = "unknown cause"

				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", msg),
				)
			}
		}
	}()
	err := twm.doer.Do(ctx, workerID)
	phase = taskPhaseInvalid // done, no panic occurred
	if err == nil {
		respFlags.Passed = 1
		return
	}

	lt.logger.LogAttrs(ctx, slog.LevelWarn,
		"task error",
		slog.Int("worker_id", workerID),
		slog.Any("error", err),
	)

	respFlags.Errored = 1

	return
}

func (lt *Loadtest) doTask_retriesDisabled_metricsEnabled_taskMetadataProviderDisabled(ctx context.Context, workerID int, twm taskWithMeta) {

	var respFlags taskResultFlags
	{
		taskStart := time.Now()
		defer func() {
			taskEnd := time.Now()

			lt.resultsChan <- taskResult{
				taskResultFlags: respFlags,
				QueueDuration:   taskStart.Sub(twm.enqueueTime),
				TaskDuration:    taskEnd.Sub(taskStart),
				Meta:            twm.meta,
			}
		}()
	}

	// phase is the name of the step which has possibly caused a panic
	phase := taskPhaseDo

	defer func() {

		if r := recover(); r != nil {

			respFlags.Panicked = 1
			respFlags.Errored = 1

			const evtName = "worker recovered from panic"

			switch v := r.(type) {
			case error:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.Any("error", v),
				)
			case []byte:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", string(v)),
				)
			case string:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", v),
				)
			default:
				const msg = "unknown cause"

				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", msg),
				)
			}
		}
	}()
	err := twm.doer.Do(ctx, workerID)
	phase = taskPhaseInvalid // done, no panic occurred
	if err == nil {
		respFlags.Passed = 1
		return
	}

	lt.logger.LogAttrs(ctx, slog.LevelWarn,
		"task error",
		slog.Int("worker_id", workerID),
		slog.Any("error", err),
	)

	respFlags.Errored = 1

	return
}

func (lt *Loadtest) doTask_retriesDisabled_metricsDisabled_taskMetadataProviderEnabled(ctx context.Context, workerID int, twm taskWithMeta) {
	taskStart := time.Now()

	defer lt.resultWaitGroup.Done()

	tm := newTaskMetadata()
	defer releaseTaskMetadata(tm)

	tm.enqueueTime = twm.enqueueTime
	tm.dequeueTime = taskStart
	tm.meta = twm.meta
	ctx = injectTaskMetadataProvider(ctx, tm)

	// phase is the name of the step which has possibly caused a panic
	phase := taskPhaseDo

	defer func() {

		if r := recover(); r != nil {

			const evtName = "worker recovered from panic"

			switch v := r.(type) {
			case error:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.Any("error", v),
				)
			case []byte:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", string(v)),
				)
			case string:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", v),
				)
			default:
				const msg = "unknown cause"

				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", msg),
				)
			}
		}
	}()
	err := twm.doer.Do(ctx, workerID)
	phase = taskPhaseInvalid // done, no panic occurred
	if err == nil {

		return
	}

	lt.logger.LogAttrs(ctx, slog.LevelWarn,
		"task error",
		slog.Int("worker_id", workerID),
		slog.Any("error", err),
	)

	return
}

func (lt *Loadtest) doTask_retriesDisabled_metricsDisabled_taskMetadataProviderDisabled(ctx context.Context, workerID int, twm taskWithMeta) {

	defer lt.resultWaitGroup.Done()

	// phase is the name of the step which has possibly caused a panic
	phase := taskPhaseDo

	defer func() {

		if r := recover(); r != nil {

			const evtName = "worker recovered from panic"

			switch v := r.(type) {
			case error:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.Any("error", v),
				)
			case []byte:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", string(v)),
				)
			case string:
				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", v),
				)
			default:
				const msg = "unknown cause"

				lt.logger.LogAttrs(ctx, slog.LevelError,
					evtName,
					slog.Int("worker_id", workerID),
					slog.String("phase", phase.String()),
					slog.String("error", msg),
				)
			}
		}
	}()
	err := twm.doer.Do(ctx, workerID)
	phase = taskPhaseInvalid // done, no panic occurred
	if err == nil {

		return
	}

	lt.logger.LogAttrs(ctx, slog.LevelWarn,
		"task error",
		slog.Int("worker_id", workerID),
		slog.Any("error", err),
	)

	return
}

func (lt *Loadtest) run_retriesEnabled_maxTasksGTZero_metricsEnabled(ctx context.Context, shutdownErrResp *error) error {

	cfgUpdateChan := lt.cfgUpdateChan
	defer close(cfgUpdateChan)

	lt.startTime = time.Now()

	{

		csvFile, err := os.Create(lt.csvData.outputFilename)
		if err != nil {
			return fmt.Errorf("failed to open output csv metrics file for writing: %w", err)
		}
		defer lt.writeOutputCsvFooterAndClose(csvFile)

		lt.csvData.writeErr = lt.writeOutputCsvConfigComment(csvFile)

		if lt.csvData.writeErr == nil {

			lt.csvData.writer = csv.NewWriter(csvFile)

			lt.csvData.writeErr = lt.writeOutputCsvHeaders()
		}
	}

	lt.logger.LogAttrs(ctx, slog.LevelInfo,
		"starting loadtest",
		slog.Any("config", lt.loadtestConfigAsJson()),
	)

	var wg sync.WaitGroup

	wg.Go(lt.resultsHandler)

	numWorkers := lt.numWorkers
	numSpawnedWorkers := 0

	// numTasks is the total number of tasks
	// scheduled to run ( including retries )
	var numTasks int

	intervalID := time.Now()

	maxTasks := lt.maxTasks

	interval := lt.interval
	numNewTasks := lt.numIntervalTasks
	ctxDone := ctx.Done()
	taskReader := lt.taskReader
	configChanges := make([]slog.Attr, 0, 6)
	meta := taskMeta{
		NumIntervalTasks: lt.numIntervalTasks,
	}
	var interTaskInterval time.Duration
	if meta.NumIntervalTasks > 0 {
		interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
	}

	taskBuf := make([]Doer, 0, lt.maxIntervalTasks)

	var enqueueTasks func()
	var updateEnqueueTasksStrategy func()
	{
		floodStrategy := func() {
			for _, task := range taskBuf {
				lt.taskChan <- taskWithMeta{task, intervalID, meta}
			}
		}

		staggerStrategy := func() {
			lt.taskChan <- taskWithMeta{taskBuf[0], intervalID, meta}

			for _, task := range taskBuf[1:] {
				time.Sleep(interTaskInterval)
				lt.taskChan <- taskWithMeta{task, time.Now(), meta}
			}
		}

		updateEnqueueTasksStrategy = func() {
			if interTaskInterval <= skipInterTaskSchedulingThreshold {
				enqueueTasks = floodStrategy
			} else {
				enqueueTasks = staggerStrategy
			}
		}
	}
	updateEnqueueTasksStrategy()

	var delay time.Duration

	readRetries := func(p []Doer) int {
		// make sure you only fill up to len

		var i int
		for i < len(p) {
			select {
			case task := <-lt.retryTaskChan:
				p[i] = task
			default:
				return i
			}
			i++
		}

		return i
	}

	// stopping routine runs on return
	// flushing as much as possible
	defer func() {

		err := func(flushRetries bool) error {
			if !flushRetries {

				lt.logger.LogAttrs(ctx, slog.LevelDebug,
					"not waiting on retries to flush on shutdown",
					slog.String("reason", "retries disabled or flush retries on shutdown disabled"),
					slog.Int("num_tasks", numTasks),
				)

				return nil
			}

			if err := ctx.Err(); err != nil {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"not waiting on retries to flush on shutdown",
					slog.String("reason", "user stopped loadtest"),
					slog.Int("num_tasks", numTasks),
					slog.Any("error", err),
				)
				return nil
			}

			lt.logger.LogAttrs(ctx, slog.LevelDebug,
				"waiting on retries to flush",
				slog.Int("num_tasks", numTasks),
			)

			if meta.NumIntervalTasks <= 0 || numWorkers <= 0 {

				lt.logger.LogAttrs(ctx, slog.LevelError,
					"retry flushing could not be attempted",
					slog.Int("num_tasks", numTasks),
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
				)

				return ErrRetriesFailedToFlush
			}

			preflushNumTasks := numTasks

			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"shutting down: flushing retries",
				slog.Int("num_tasks", numTasks),
				slog.String("flush_retries_timeout", lt.flushRetriesTimeout.String()),
			)

			shutdownCtx, cancel := context.WithTimeout(context.Background(), lt.flushRetriesTimeout)
			defer cancel()

			intervalID = time.Now()
			taskBuf = taskBuf[:0]
			meta.Lag = 0

			for {

				if err := shutdownCtx.Err(); err != nil {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"failed to flush all retries",
						slog.Int("preflush_num_tasks", preflushNumTasks),
						slog.Int("num_tasks", numTasks),
						slog.Any("error", err),
					)

					return ErrRetriesFailedToFlush
				}

				lt.resultWaitGroup.Wait()

				for {

					if err := shutdownCtx.Err(); err != nil {
						lt.logger.LogAttrs(ctx, slog.LevelError,
							"failed to flush all retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
							slog.Any("error", err),
						)

						return ErrRetriesFailedToFlush
					}

					if numTasks >= maxTasks {
						lt.logger.LogAttrs(ctx, slog.LevelError,
							"failed to flush all retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
							slog.String("reason", "reached max tasks"),
						)
						return ErrRetriesFailedToFlush
					}

					// 1. the below looks off/odd, why not use?:
					//
					// ```
					// if n := maxTasks - numTasks; n < numNewTasks {
					// 	numNewTasks = n
					// }
					// ```
					//
					// 2. And for that matter, why not keep meta.NumIntervalTasks in sync with numNewTasks?
					//
					// ---
					//
					// 1. The implementation would be exactly the same, just using another variable
					// 2. the meta.NumIntervalTasks value is used in RATE calculations, if we keep it in sync
					//    with BOUNDS values then the last tasks could run at a lower RATE than intended. It
					//    is only kept in sync when a user adjusts the RATE via a ConfigUpdate. Don't confuse
					//    bounds purpose values with rate purpose values.
					//
					numNewTasks = maxTasks - numTasks
					if numNewTasks > meta.NumIntervalTasks {
						numNewTasks = meta.NumIntervalTasks
					}

					select {
					case <-ctxDone:
						lt.logger.LogAttrs(ctx, slog.LevelWarn,
							"user stopped loadtest while attempting to flush retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
						)
						return nil
					default:
						// continue with load generating retries
					}

					// acquire load generation opportunity slots ( smooths bursts )
					//
					// in the shutdown retry flow we always want to acquire before reading retries
					// to avoid a deadlock edge case of the retry queue being full, all workers tasks failed and need to be retried
					if err := lt.intervalTasksSema.Acquire(shutdownCtx, int64(numNewTasks)); err != nil {
						lt.logger.LogAttrs(ctx, slog.LevelError,
							"failed to flush all retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
							slog.Any("error", err),
							slog.String("reason", "shutdown timeout likely reached while waiting for semaphore acquisition"),
						)
						return ErrRetriesFailedToFlush
					}

					// read up to numNewTasks from retry slice
					taskBufSize := readRetries(taskBuf[:numNewTasks:numNewTasks])
					if taskBufSize <= 0 {
						// wait for any pending tasks to flush and try read again

						lt.logger.LogAttrs(ctx, slog.LevelDebug,
							"verifying all retries have flushed",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
						)

						lt.resultWaitGroup.Wait()

						// read up to numNewTasks from retry slice again
						taskBufSize = readRetries(taskBuf[:numNewTasks:numNewTasks])
						if taskBufSize <= 0 {

							lt.logger.LogAttrs(ctx, slog.LevelInfo,
								"all retries flushed",
								slog.Int("preflush_num_tasks", preflushNumTasks),
								slog.Int("num_tasks", numTasks),
							)
							return nil
						}
					}
					taskBuf = taskBuf[:taskBufSize]

					// re-release any extra load slots we allocated beyond what really remains to do
					if numNewTasks > taskBufSize {
						lt.intervalTasksSema.Release(int64(numNewTasks - taskBufSize))
					}

					lt.resultWaitGroup.Add(taskBufSize + 1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
					lt.resultsChan <- taskResult{
						Meta: taskMeta{
							// IntervalID: intervalID, // not required unless in a debug context
							SampleSize: taskBufSize,
						},
					}

					meta.IntervalID = intervalID

					enqueueTasks()

					taskBuf = taskBuf[:0]

					numTasks += taskBufSize

					meta.Lag = 0

					// wait for next interval time to exist
					nextIntervalID := intervalID.Add(interval)
					realNow := time.Now()
					delay = nextIntervalID.Sub(realNow)
					if delay > 0 {
						time.Sleep(delay)
						intervalID = nextIntervalID

						if taskBufSize < numNewTasks {
							// just finished this iteration of retry enqueuing
							//
							// break to loop through retry drain context again
							break
						}

						continue
					}

					if delay < 0 {
						intervalID = realNow

						lag := -delay
						meta.Lag = lag

						lt.resultWaitGroup.Add(1)
						lt.resultsChan <- taskResult{
							Meta: taskMeta{
								// IntervalID: intervalID, // not required unless in a debug context
								Lag: lag,
							},
						}

					}

					if taskBufSize < numNewTasks {
						// just finished this iteration of retry enqueuing
						//
						// break to loop through retry drain context again
						break
					}
				}
			}
		}(lt.flushRetriesOnShutdown)
		if err != nil {
			*shutdownErrResp = err
		}

		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for running tasks to stop",
		)
		lt.resultWaitGroup.Wait()

		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping result handler routine",
		)

		// signal for result handler routines to stop
		close(lt.resultsChan)

		// signal for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping workers",
		)
		for i := 0; i < numSpawnedWorkers; i++ {
			close(lt.pauseChans[i])
		}

		// wait for result handler routines to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for result handler routines to stop",
		)
		wg.Wait()

		// wait for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for workers to stop",
		)
		lt.workerWaitGroup.Wait()

		lt.logger.LogAttrs(ctx, slog.LevelInfo,
			"loadtest stopped",
		)
	}()

	// getTaskSlotCount is the task emission back pressure
	// throttle that conveys the number of tasks that
	// are allowed to be un-finished for the performance
	// interval under normal circumstances
	getTaskSlotCount := func() int {
		return maxPendingTasks(numWorkers, numNewTasks)
	}

	// apply initial task buffer limits to the interval semaphore
	taskSlotCount := getTaskSlotCount()
	lt.intervalTasksSema.Release(int64(taskSlotCount))

	configCausesPause := func() bool {
		return meta.NumIntervalTasks <= 0 || numWorkers <= 0
	}

	var paused bool
	var pauseStart time.Time

	handleConfigUpdateAndPauseState := func(cu ConfigUpdate) error {
		for {
			var prepSemaErr error
			var recomputeInterTaskInterval, recomputeTaskSlots bool

			if cu.numWorkers.set {
				recomputeTaskSlots = true

				n := cu.numWorkers.val

				// prevent over committing on the maxWorkers count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxWorkers {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too high"),
						slog.String("remediation_hint", "increase the loadtest MaxWorkers setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxWorkers),
					)
					n = lt.maxWorkers
				}

				if n > numWorkers {

					// unpause workers
					for i := numWorkers; i < numSpawnedWorkers; i++ {
						lt.pauseChans[i] <- struct{}{}
					}

					// spawn new workers if needed
					for i := numSpawnedWorkers; i < n; i++ {
						lt.addWorker(ctx, i)
						numSpawnedWorkers++
					}
				} else if n < numWorkers {

					// pause workers if needed
					for i := numWorkers - 1; i >= n; i-- {
						lt.pauseChans[i] <- struct{}{}
					}
				}

				configChanges = append(configChanges,
					slog.Int("old_num_workers", numWorkers),
					slog.Int("new_num_workers", n),
				)
				numWorkers = n
			}

			if cu.numIntervalTasks.set {
				recomputeInterTaskInterval = true
				recomputeTaskSlots = true

				n := cu.numIntervalTasks.val

				// prevent over committing on the maxIntervalTasks count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxIntervalTasks {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too high"),
						slog.String("remediation_hint", "increase the loadtest MaxIntervalTasks setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxIntervalTasks),
					)
					n = lt.maxIntervalTasks
				}

				configChanges = append(configChanges,
					slog.Int("old_num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("new_num_interval_tasks", n),
				)
				numNewTasks = n
				meta.NumIntervalTasks = n
			}

			if cu.interval.set {
				recomputeInterTaskInterval = true

				n := cu.interval.val

				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: interval",
						slog.String("reason", "update tried to set interval too low"),
						slog.String("remediation_taken", "using min value"),
						slog.String("requested", n.String()),
						slog.String("min", time.Duration(0).String()),
					)
					n = 0
				}

				configChanges = append(configChanges,
					slog.String("old_interval", interval.String()),
					slog.String("new_interval", n.String()),
				)
				interval = n
			}

			// && clause: protects against divide by zero
			if recomputeInterTaskInterval && meta.NumIntervalTasks > 0 {
				interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
				updateEnqueueTasksStrategy()
			}

			if recomputeTaskSlots {
				if newTaskSlotCount := getTaskSlotCount(); newTaskSlotCount != taskSlotCount {

					if newTaskSlotCount > taskSlotCount {
						lt.intervalTasksSema.Release(int64(newTaskSlotCount - taskSlotCount))
					} else {
						prepSemaErr = lt.intervalTasksSema.Acquire(ctx, int64(taskSlotCount-newTaskSlotCount))
						if prepSemaErr != nil {
							lt.logger.LogAttrs(ctx, slog.LevelError,
								"loadtest config update: failed to pre-acquire load generation slots",
								slog.Any("error", prepSemaErr),
							)

							// not returning and error... yet
							// going to let config update log statement occur and then report the error present in prepSemaErr
						}
					}

					taskSlotCount = newTaskSlotCount
				}
			}

			if !cu.onStartup {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"loadtest config updated",
					configChanges...,
				)
			}
			configChanges = configChanges[:0]

			if prepSemaErr != nil {
				return prepSemaErr
			}

			// pause load generation if unable to schedule anything
			if configCausesPause() {

				if !paused {
					paused = true
					pauseStart = time.Now().UTC()

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"pausing load generation",
						slog.Int("num_interval_tasks", meta.NumIntervalTasks),
						slog.Int("num_workers", numWorkers),
						slog.String("paused_at", pauseStart.String()),
					)
				}

				// duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
				// that follows
				//
				// ref: https://go.dev/ref/spec#Select_statements
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				default:
				}
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				case cu = <-cfgUpdateChan:
					continue
				}
			}

			if paused {
				paused = false
				intervalID = time.Now()

				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"resuming load generation",
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
					slog.String("paused_at", pauseStart.String()),
					slog.String("resumed_at", intervalID.UTC().String()),
				)
			}

			return nil
		}
	}

	if configCausesPause() {
		if err := handleConfigUpdateAndPauseState(ConfigUpdate{onStartup: true}); err != nil {
			if err == errLoadtestContextDone {
				return nil
			}
			return err
		}
	}

	// start workers just before starting task scheduling
	for i := 0; i < numWorkers; i++ {
		lt.addWorker(ctx, i)
		numSpawnedWorkers++
	}

	// main task scheduling loop
	for {
		if numTasks >= maxTasks {
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"loadtest finished: max task count reached",
				slog.Int("max_tasks", maxTasks),
			)
			return nil
		}

		numNewTasks = maxTasks - numTasks
		if numNewTasks > meta.NumIntervalTasks {
			numNewTasks = meta.NumIntervalTasks
		}

		// Not duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
		// that follows because this is a ordered sequence where all selects in the sequence have non-blocking default cases.
		//
		// The odds of thread switching between these selects is minimal as is the impact of having to wait at most one more cycle
		// to short circuit and return.
		//
		// ref: https://go.dev/ref/spec#Select_statements
		select {
		case <-ctxDone:
			return nil
		default:
		}
		select {
		case cu := <-cfgUpdateChan:
			if err := handleConfigUpdateAndPauseState(cu); err != nil {
				if err == errLoadtestContextDone {
					return nil
				}
				return err
			}

			// re-loop
			continue
		default:
			// continue with load generation
		}

		taskBufSize := 0

		//
		// read up to numNewTasks from retry slice
		//

		// acquire load generation opportunity slots ( smooths bursts )
		//
		// do this early conditionally to allow retries to settle in the retry channel
		// so we can pick them up when enough buffer space has cleared
		//
		// thus we avoid a possible deadlock where the retry queue is full and the workers
		// all have failed tasks that wish to be retried
		if lt.intervalTasksSema.Acquire(ctx, int64(numNewTasks)) != nil {
			return nil
		}

		taskBufSize = readRetries(taskBuf[:numNewTasks:numNewTasks])

		taskBuf = taskBuf[:taskBufSize]

		if taskBufSize < numNewTasks {
			maxSize := numNewTasks - taskBufSize
			n := taskReader.ReadTasks(taskBuf[taskBufSize:numNewTasks:numNewTasks])
			if n < 0 || n > maxSize {
				panic(ErrBadReadTasksImpl)
			}
			if n == 0 {

				// iteration is technically done now
				// but there could be straggling retries
				// queued after this, those should continue
				// to be flushed if and only if maxTasks
				// has not been reached and if it is greater
				// than zero
				if taskBufSize == 0 {
					// return immediately if there is nothing
					// new to enqueue

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"stopping loadtest: ReadTasks did not load enough tasks",
						slog.Int("final_task_delta", 0),
					)

					return nil
				}

				lt.logger.LogAttrs(ctx, slog.LevelDebug,
					"scheduled: stopping loadtest: ReadTasks did not load enough tasks",
					slog.Int("retry_count", taskBufSize),
				)
			}

			taskBufSize += n
			taskBuf = taskBuf[:taskBufSize]
		}

		// acquire load generation opportunity slots ( smooths bursts )
		// if not done already
		//
		// but if we allocated too many in our retry prep phase then release the
		// difference
		if numNewTasks > taskBufSize {
			lt.intervalTasksSema.Release(int64(numNewTasks - taskBufSize))
		}

		lt.resultWaitGroup.Add(taskBufSize + 1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
		lt.resultsChan <- taskResult{
			Meta: taskMeta{
				// IntervalID: intervalID, // not required unless in a debug context
				SampleSize: taskBufSize,
			},
		}

		meta.IntervalID = intervalID

		enqueueTasks()

		if numNewTasks > taskBufSize {
			// must have hit the end of ReadTasks iterator
			// increase numTasks total by actual number queued
			// and stop traffic generation
			numTasks += taskBufSize
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"stopping loadtest: ReadTasks did not load enough tasks",
				slog.Int("final_task_delta", taskBufSize),
			)
			return nil
		}

		taskBuf = taskBuf[:0]

		numTasks += taskBufSize

		meta.Lag = 0

		// wait for next interval time to exist
		nextIntervalID := intervalID.Add(interval)
		realNow := time.Now()
		delay = nextIntervalID.Sub(realNow)
		if delay > 0 {
			time.Sleep(delay)
			intervalID = nextIntervalID
			continue
		}

		if delay < 0 {
			intervalID = realNow

			lag := -delay
			meta.Lag = lag

			lt.resultWaitGroup.Add(1)
			lt.resultsChan <- taskResult{
				Meta: taskMeta{
					// IntervalID: intervalID, // not required unless in a debug context
					Lag: lag,
				},
			}

		}
	}
}

func (lt *Loadtest) run_retriesEnabled_maxTasksGTZero_metricsDisabled(ctx context.Context, shutdownErrResp *error) error {

	cfgUpdateChan := lt.cfgUpdateChan
	defer close(cfgUpdateChan)

	lt.startTime = time.Now()

	lt.logger.LogAttrs(ctx, slog.LevelInfo,
		"starting loadtest",
		slog.Any("config", lt.loadtestConfigAsJson()),
	)

	numWorkers := lt.numWorkers
	numSpawnedWorkers := 0

	// numTasks is the total number of tasks
	// scheduled to run ( including retries )
	var numTasks int

	intervalID := time.Now()

	maxTasks := lt.maxTasks

	interval := lt.interval
	numNewTasks := lt.numIntervalTasks
	ctxDone := ctx.Done()
	taskReader := lt.taskReader
	configChanges := make([]slog.Attr, 0, 6)
	meta := taskMeta{
		NumIntervalTasks: lt.numIntervalTasks,
	}
	var interTaskInterval time.Duration
	if meta.NumIntervalTasks > 0 {
		interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
	}

	taskBuf := make([]Doer, 0, lt.maxIntervalTasks)

	var enqueueTasks func()
	var updateEnqueueTasksStrategy func()
	{
		floodStrategy := func() {
			for _, task := range taskBuf {
				lt.taskChan <- taskWithMeta{task, intervalID, meta}
			}
		}

		staggerStrategy := func() {
			lt.taskChan <- taskWithMeta{taskBuf[0], intervalID, meta}

			for _, task := range taskBuf[1:] {
				time.Sleep(interTaskInterval)
				lt.taskChan <- taskWithMeta{task, time.Now(), meta}
			}
		}

		updateEnqueueTasksStrategy = func() {
			if interTaskInterval <= skipInterTaskSchedulingThreshold {
				enqueueTasks = floodStrategy
			} else {
				enqueueTasks = staggerStrategy
			}
		}
	}
	updateEnqueueTasksStrategy()

	var delay time.Duration

	readRetries := func(p []Doer) int {
		// make sure you only fill up to len

		var i int
		for i < len(p) {
			select {
			case task := <-lt.retryTaskChan:
				p[i] = task
			default:
				return i
			}
			i++
		}

		return i
	}

	// stopping routine runs on return
	// flushing as much as possible
	defer func() {

		err := func(flushRetries bool) error {
			if !flushRetries {

				lt.logger.LogAttrs(ctx, slog.LevelDebug,
					"not waiting on retries to flush on shutdown",
					slog.String("reason", "retries disabled or flush retries on shutdown disabled"),
					slog.Int("num_tasks", numTasks),
				)

				return nil
			}

			if err := ctx.Err(); err != nil {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"not waiting on retries to flush on shutdown",
					slog.String("reason", "user stopped loadtest"),
					slog.Int("num_tasks", numTasks),
					slog.Any("error", err),
				)
				return nil
			}

			lt.logger.LogAttrs(ctx, slog.LevelDebug,
				"waiting on retries to flush",
				slog.Int("num_tasks", numTasks),
			)

			if meta.NumIntervalTasks <= 0 || numWorkers <= 0 {

				lt.logger.LogAttrs(ctx, slog.LevelError,
					"retry flushing could not be attempted",
					slog.Int("num_tasks", numTasks),
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
				)

				return ErrRetriesFailedToFlush
			}

			preflushNumTasks := numTasks

			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"shutting down: flushing retries",
				slog.Int("num_tasks", numTasks),
				slog.String("flush_retries_timeout", lt.flushRetriesTimeout.String()),
			)

			shutdownCtx, cancel := context.WithTimeout(context.Background(), lt.flushRetriesTimeout)
			defer cancel()

			intervalID = time.Now()
			taskBuf = taskBuf[:0]

			for {

				if err := shutdownCtx.Err(); err != nil {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"failed to flush all retries",
						slog.Int("preflush_num_tasks", preflushNumTasks),
						slog.Int("num_tasks", numTasks),
						slog.Any("error", err),
					)

					return ErrRetriesFailedToFlush
				}

				lt.resultWaitGroup.Wait()

				for {

					if err := shutdownCtx.Err(); err != nil {
						lt.logger.LogAttrs(ctx, slog.LevelError,
							"failed to flush all retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
							slog.Any("error", err),
						)

						return ErrRetriesFailedToFlush
					}

					if numTasks >= maxTasks {
						lt.logger.LogAttrs(ctx, slog.LevelError,
							"failed to flush all retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
							slog.String("reason", "reached max tasks"),
						)
						return ErrRetriesFailedToFlush
					}

					// 1. the below looks off/odd, why not use?:
					//
					// ```
					// if n := maxTasks - numTasks; n < numNewTasks {
					// 	numNewTasks = n
					// }
					// ```
					//
					// 2. And for that matter, why not keep meta.NumIntervalTasks in sync with numNewTasks?
					//
					// ---
					//
					// 1. The implementation would be exactly the same, just using another variable
					// 2. the meta.NumIntervalTasks value is used in RATE calculations, if we keep it in sync
					//    with BOUNDS values then the last tasks could run at a lower RATE than intended. It
					//    is only kept in sync when a user adjusts the RATE via a ConfigUpdate. Don't confuse
					//    bounds purpose values with rate purpose values.
					//
					numNewTasks = maxTasks - numTasks
					if numNewTasks > meta.NumIntervalTasks {
						numNewTasks = meta.NumIntervalTasks
					}

					select {
					case <-ctxDone:
						lt.logger.LogAttrs(ctx, slog.LevelWarn,
							"user stopped loadtest while attempting to flush retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
						)
						return nil
					default:
						// continue with load generating retries
					}

					// acquire load generation opportunity slots ( smooths bursts )
					//
					// in the shutdown retry flow we always want to acquire before reading retries
					// to avoid a deadlock edge case of the retry queue being full, all workers tasks failed and need to be retried
					if err := lt.intervalTasksSema.Acquire(shutdownCtx, int64(numNewTasks)); err != nil {
						lt.logger.LogAttrs(ctx, slog.LevelError,
							"failed to flush all retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
							slog.Any("error", err),
							slog.String("reason", "shutdown timeout likely reached while waiting for semaphore acquisition"),
						)
						return ErrRetriesFailedToFlush
					}

					// read up to numNewTasks from retry slice
					taskBufSize := readRetries(taskBuf[:numNewTasks:numNewTasks])
					if taskBufSize <= 0 {
						// wait for any pending tasks to flush and try read again

						lt.logger.LogAttrs(ctx, slog.LevelDebug,
							"verifying all retries have flushed",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
						)

						lt.resultWaitGroup.Wait()

						// read up to numNewTasks from retry slice again
						taskBufSize = readRetries(taskBuf[:numNewTasks:numNewTasks])
						if taskBufSize <= 0 {

							lt.logger.LogAttrs(ctx, slog.LevelInfo,
								"all retries flushed",
								slog.Int("preflush_num_tasks", preflushNumTasks),
								slog.Int("num_tasks", numTasks),
							)
							return nil
						}
					}
					taskBuf = taskBuf[:taskBufSize]

					// re-release any extra load slots we allocated beyond what really remains to do
					if numNewTasks > taskBufSize {
						lt.intervalTasksSema.Release(int64(numNewTasks - taskBufSize))
					}

					lt.resultWaitGroup.Add(taskBufSize + 1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
					lt.resultsChan <- taskResult{
						Meta: taskMeta{
							// IntervalID: intervalID, // not required unless in a debug context
							SampleSize: taskBufSize,
						},
					}

					meta.IntervalID = intervalID

					enqueueTasks()

					taskBuf = taskBuf[:0]

					numTasks += taskBufSize

					// wait for next interval time to exist
					nextIntervalID := intervalID.Add(interval)
					realNow := time.Now()
					delay = nextIntervalID.Sub(realNow)
					if delay > 0 {
						time.Sleep(delay)
						intervalID = nextIntervalID

						if taskBufSize < numNewTasks {
							// just finished this iteration of retry enqueuing
							//
							// break to loop through retry drain context again
							break
						}

						continue
					}

					if delay < 0 {
						intervalID = realNow

					}

					if taskBufSize < numNewTasks {
						// just finished this iteration of retry enqueuing
						//
						// break to loop through retry drain context again
						break
					}
				}
			}
		}(lt.flushRetriesOnShutdown)
		if err != nil {
			*shutdownErrResp = err
		}

		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for running tasks to stop",
		)
		lt.resultWaitGroup.Wait()

		// signal for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping workers",
		)
		for i := 0; i < numSpawnedWorkers; i++ {
			close(lt.pauseChans[i])
		}

		// wait for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for workers to stop",
		)
		lt.workerWaitGroup.Wait()

		lt.logger.LogAttrs(ctx, slog.LevelInfo,
			"loadtest stopped",
		)
	}()

	// getTaskSlotCount is the task emission back pressure
	// throttle that conveys the number of tasks that
	// are allowed to be un-finished for the performance
	// interval under normal circumstances
	getTaskSlotCount := func() int {
		return maxPendingTasks(numWorkers, numNewTasks)
	}

	// apply initial task buffer limits to the interval semaphore
	taskSlotCount := getTaskSlotCount()
	lt.intervalTasksSema.Release(int64(taskSlotCount))

	configCausesPause := func() bool {
		return meta.NumIntervalTasks <= 0 || numWorkers <= 0
	}

	var paused bool
	var pauseStart time.Time

	handleConfigUpdateAndPauseState := func(cu ConfigUpdate) error {
		for {
			var prepSemaErr error
			var recomputeInterTaskInterval, recomputeTaskSlots bool

			if cu.numWorkers.set {
				recomputeTaskSlots = true

				n := cu.numWorkers.val

				// prevent over committing on the maxWorkers count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxWorkers {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too high"),
						slog.String("remediation_hint", "increase the loadtest MaxWorkers setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxWorkers),
					)
					n = lt.maxWorkers
				}

				if n > numWorkers {

					// unpause workers
					for i := numWorkers; i < numSpawnedWorkers; i++ {
						lt.pauseChans[i] <- struct{}{}
					}

					// spawn new workers if needed
					for i := numSpawnedWorkers; i < n; i++ {
						lt.addWorker(ctx, i)
						numSpawnedWorkers++
					}
				} else if n < numWorkers {

					// pause workers if needed
					for i := numWorkers - 1; i >= n; i-- {
						lt.pauseChans[i] <- struct{}{}
					}
				}

				configChanges = append(configChanges,
					slog.Int("old_num_workers", numWorkers),
					slog.Int("new_num_workers", n),
				)
				numWorkers = n
			}

			if cu.numIntervalTasks.set {
				recomputeInterTaskInterval = true
				recomputeTaskSlots = true

				n := cu.numIntervalTasks.val

				// prevent over committing on the maxIntervalTasks count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxIntervalTasks {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too high"),
						slog.String("remediation_hint", "increase the loadtest MaxIntervalTasks setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxIntervalTasks),
					)
					n = lt.maxIntervalTasks
				}

				configChanges = append(configChanges,
					slog.Int("old_num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("new_num_interval_tasks", n),
				)
				numNewTasks = n
				meta.NumIntervalTasks = n
			}

			if cu.interval.set {
				recomputeInterTaskInterval = true

				n := cu.interval.val

				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: interval",
						slog.String("reason", "update tried to set interval too low"),
						slog.String("remediation_taken", "using min value"),
						slog.String("requested", n.String()),
						slog.String("min", time.Duration(0).String()),
					)
					n = 0
				}

				configChanges = append(configChanges,
					slog.String("old_interval", interval.String()),
					slog.String("new_interval", n.String()),
				)
				interval = n
			}

			// && clause: protects against divide by zero
			if recomputeInterTaskInterval && meta.NumIntervalTasks > 0 {
				interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
				updateEnqueueTasksStrategy()
			}

			if recomputeTaskSlots {
				if newTaskSlotCount := getTaskSlotCount(); newTaskSlotCount != taskSlotCount {

					if newTaskSlotCount > taskSlotCount {
						lt.intervalTasksSema.Release(int64(newTaskSlotCount - taskSlotCount))
					} else {
						prepSemaErr = lt.intervalTasksSema.Acquire(ctx, int64(taskSlotCount-newTaskSlotCount))
						if prepSemaErr != nil {
							lt.logger.LogAttrs(ctx, slog.LevelError,
								"loadtest config update: failed to pre-acquire load generation slots",
								slog.Any("error", prepSemaErr),
							)

							// not returning and error... yet
							// going to let config update log statement occur and then report the error present in prepSemaErr
						}
					}

					taskSlotCount = newTaskSlotCount
				}
			}

			if !cu.onStartup {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"loadtest config updated",
					configChanges...,
				)
			}
			configChanges = configChanges[:0]

			if prepSemaErr != nil {
				return prepSemaErr
			}

			// pause load generation if unable to schedule anything
			if configCausesPause() {

				if !paused {
					paused = true
					pauseStart = time.Now().UTC()

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"pausing load generation",
						slog.Int("num_interval_tasks", meta.NumIntervalTasks),
						slog.Int("num_workers", numWorkers),
						slog.String("paused_at", pauseStart.String()),
					)
				}

				// duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
				// that follows
				//
				// ref: https://go.dev/ref/spec#Select_statements
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				default:
				}
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				case cu = <-cfgUpdateChan:
					continue
				}
			}

			if paused {
				paused = false
				intervalID = time.Now()

				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"resuming load generation",
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
					slog.String("paused_at", pauseStart.String()),
					slog.String("resumed_at", intervalID.UTC().String()),
				)
			}

			return nil
		}
	}

	if configCausesPause() {
		if err := handleConfigUpdateAndPauseState(ConfigUpdate{onStartup: true}); err != nil {
			if err == errLoadtestContextDone {
				return nil
			}
			return err
		}
	}

	// start workers just before starting task scheduling
	for i := 0; i < numWorkers; i++ {
		lt.addWorker(ctx, i)
		numSpawnedWorkers++
	}

	// main task scheduling loop
	for {
		if numTasks >= maxTasks {
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"loadtest finished: max task count reached",
				slog.Int("max_tasks", maxTasks),
			)
			return nil
		}

		numNewTasks = maxTasks - numTasks
		if numNewTasks > meta.NumIntervalTasks {
			numNewTasks = meta.NumIntervalTasks
		}

		// Not duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
		// that follows because this is a ordered sequence where all selects in the sequence have non-blocking default cases.
		//
		// The odds of thread switching between these selects is minimal as is the impact of having to wait at most one more cycle
		// to short circuit and return.
		//
		// ref: https://go.dev/ref/spec#Select_statements
		select {
		case <-ctxDone:
			return nil
		default:
		}
		select {
		case cu := <-cfgUpdateChan:
			if err := handleConfigUpdateAndPauseState(cu); err != nil {
				if err == errLoadtestContextDone {
					return nil
				}
				return err
			}

			// re-loop
			continue
		default:
			// continue with load generation
		}

		taskBufSize := 0

		//
		// read up to numNewTasks from retry slice
		//

		// acquire load generation opportunity slots ( smooths bursts )
		//
		// do this early conditionally to allow retries to settle in the retry channel
		// so we can pick them up when enough buffer space has cleared
		//
		// thus we avoid a possible deadlock where the retry queue is full and the workers
		// all have failed tasks that wish to be retried
		if lt.intervalTasksSema.Acquire(ctx, int64(numNewTasks)) != nil {
			return nil
		}

		taskBufSize = readRetries(taskBuf[:numNewTasks:numNewTasks])

		taskBuf = taskBuf[:taskBufSize]

		if taskBufSize < numNewTasks {
			maxSize := numNewTasks - taskBufSize
			n := taskReader.ReadTasks(taskBuf[taskBufSize:numNewTasks:numNewTasks])
			if n < 0 || n > maxSize {
				panic(ErrBadReadTasksImpl)
			}
			if n == 0 {

				// iteration is technically done now
				// but there could be straggling retries
				// queued after this, those should continue
				// to be flushed if and only if maxTasks
				// has not been reached and if it is greater
				// than zero
				if taskBufSize == 0 {
					// return immediately if there is nothing
					// new to enqueue

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"stopping loadtest: ReadTasks did not load enough tasks",
						slog.Int("final_task_delta", 0),
					)

					return nil
				}

				lt.logger.LogAttrs(ctx, slog.LevelDebug,
					"scheduled: stopping loadtest: ReadTasks did not load enough tasks",
					slog.Int("retry_count", taskBufSize),
				)
			}

			taskBufSize += n
			taskBuf = taskBuf[:taskBufSize]
		}

		// acquire load generation opportunity slots ( smooths bursts )
		// if not done already
		//
		// but if we allocated too many in our retry prep phase then release the
		// difference
		if numNewTasks > taskBufSize {
			lt.intervalTasksSema.Release(int64(numNewTasks - taskBufSize))
		}

		lt.resultWaitGroup.Add(taskBufSize + 1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
		lt.resultsChan <- taskResult{
			Meta: taskMeta{
				// IntervalID: intervalID, // not required unless in a debug context
				SampleSize: taskBufSize,
			},
		}

		meta.IntervalID = intervalID

		enqueueTasks()

		if numNewTasks > taskBufSize {
			// must have hit the end of ReadTasks iterator
			// increase numTasks total by actual number queued
			// and stop traffic generation
			numTasks += taskBufSize
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"stopping loadtest: ReadTasks did not load enough tasks",
				slog.Int("final_task_delta", taskBufSize),
			)
			return nil
		}

		taskBuf = taskBuf[:0]

		numTasks += taskBufSize

		// wait for next interval time to exist
		nextIntervalID := intervalID.Add(interval)
		realNow := time.Now()
		delay = nextIntervalID.Sub(realNow)
		if delay > 0 {
			time.Sleep(delay)
			intervalID = nextIntervalID
			continue
		}

		if delay < 0 {
			intervalID = realNow

		}
	}
}

func (lt *Loadtest) run_retriesEnabled_maxTasksNotGTZero_metricsEnabled(ctx context.Context, shutdownErrResp *error) error {

	cfgUpdateChan := lt.cfgUpdateChan
	defer close(cfgUpdateChan)

	lt.startTime = time.Now()

	{

		csvFile, err := os.Create(lt.csvData.outputFilename)
		if err != nil {
			return fmt.Errorf("failed to open output csv metrics file for writing: %w", err)
		}
		defer lt.writeOutputCsvFooterAndClose(csvFile)

		lt.csvData.writeErr = lt.writeOutputCsvConfigComment(csvFile)

		if lt.csvData.writeErr == nil {

			lt.csvData.writer = csv.NewWriter(csvFile)

			lt.csvData.writeErr = lt.writeOutputCsvHeaders()
		}
	}

	lt.logger.LogAttrs(ctx, slog.LevelInfo,
		"starting loadtest",
		slog.Any("config", lt.loadtestConfigAsJson()),
	)

	var wg sync.WaitGroup

	wg.Go(lt.resultsHandler)

	numWorkers := lt.numWorkers
	numSpawnedWorkers := 0

	// numTasks is the total number of tasks
	// scheduled to run ( including retries )
	var numTasks int

	intervalID := time.Now()

	interval := lt.interval
	numNewTasks := lt.numIntervalTasks
	ctxDone := ctx.Done()
	taskReader := lt.taskReader
	configChanges := make([]slog.Attr, 0, 6)
	meta := taskMeta{
		NumIntervalTasks: lt.numIntervalTasks,
	}
	var interTaskInterval time.Duration
	if meta.NumIntervalTasks > 0 {
		interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
	}

	taskBuf := make([]Doer, 0, lt.maxIntervalTasks)

	var enqueueTasks func()
	var updateEnqueueTasksStrategy func()
	{
		floodStrategy := func() {
			for _, task := range taskBuf {
				lt.taskChan <- taskWithMeta{task, intervalID, meta}
			}
		}

		staggerStrategy := func() {
			lt.taskChan <- taskWithMeta{taskBuf[0], intervalID, meta}

			for _, task := range taskBuf[1:] {
				time.Sleep(interTaskInterval)
				lt.taskChan <- taskWithMeta{task, time.Now(), meta}
			}
		}

		updateEnqueueTasksStrategy = func() {
			if interTaskInterval <= skipInterTaskSchedulingThreshold {
				enqueueTasks = floodStrategy
			} else {
				enqueueTasks = staggerStrategy
			}
		}
	}
	updateEnqueueTasksStrategy()

	var delay time.Duration

	readRetries := func(p []Doer) int {
		// make sure you only fill up to len

		var i int
		for i < len(p) {
			select {
			case task := <-lt.retryTaskChan:
				p[i] = task
			default:
				return i
			}
			i++
		}

		return i
	}

	// stopping routine runs on return
	// flushing as much as possible
	defer func() {

		err := func(flushRetries bool) error {
			if !flushRetries {

				lt.logger.LogAttrs(ctx, slog.LevelDebug,
					"not waiting on retries to flush on shutdown",
					slog.String("reason", "retries disabled or flush retries on shutdown disabled"),
					slog.Int("num_tasks", numTasks),
				)

				return nil
			}

			if err := ctx.Err(); err != nil {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"not waiting on retries to flush on shutdown",
					slog.String("reason", "user stopped loadtest"),
					slog.Int("num_tasks", numTasks),
					slog.Any("error", err),
				)
				return nil
			}

			lt.logger.LogAttrs(ctx, slog.LevelDebug,
				"waiting on retries to flush",
				slog.Int("num_tasks", numTasks),
			)

			if meta.NumIntervalTasks <= 0 || numWorkers <= 0 {

				lt.logger.LogAttrs(ctx, slog.LevelError,
					"retry flushing could not be attempted",
					slog.Int("num_tasks", numTasks),
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
				)

				return ErrRetriesFailedToFlush
			}

			preflushNumTasks := numTasks

			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"shutting down: flushing retries",
				slog.Int("num_tasks", numTasks),
				slog.String("flush_retries_timeout", lt.flushRetriesTimeout.String()),
			)

			shutdownCtx, cancel := context.WithTimeout(context.Background(), lt.flushRetriesTimeout)
			defer cancel()

			intervalID = time.Now()
			taskBuf = taskBuf[:0]
			meta.Lag = 0

			for {

				if err := shutdownCtx.Err(); err != nil {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"failed to flush all retries",
						slog.Int("preflush_num_tasks", preflushNumTasks),
						slog.Int("num_tasks", numTasks),
						slog.Any("error", err),
					)

					return ErrRetriesFailedToFlush
				}

				lt.resultWaitGroup.Wait()

				for {

					if err := shutdownCtx.Err(); err != nil {
						lt.logger.LogAttrs(ctx, slog.LevelError,
							"failed to flush all retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
							slog.Any("error", err),
						)

						return ErrRetriesFailedToFlush
					}

					select {
					case <-ctxDone:
						lt.logger.LogAttrs(ctx, slog.LevelWarn,
							"user stopped loadtest while attempting to flush retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
						)
						return nil
					default:
						// continue with load generating retries
					}

					// acquire load generation opportunity slots ( smooths bursts )
					//
					// in the shutdown retry flow we always want to acquire before reading retries
					// to avoid a deadlock edge case of the retry queue being full, all workers tasks failed and need to be retried
					if err := lt.intervalTasksSema.Acquire(shutdownCtx, int64(numNewTasks)); err != nil {
						lt.logger.LogAttrs(ctx, slog.LevelError,
							"failed to flush all retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
							slog.Any("error", err),
							slog.String("reason", "shutdown timeout likely reached while waiting for semaphore acquisition"),
						)
						return ErrRetriesFailedToFlush
					}

					// read up to numNewTasks from retry slice
					taskBufSize := readRetries(taskBuf[:numNewTasks:numNewTasks])
					if taskBufSize <= 0 {
						// wait for any pending tasks to flush and try read again

						lt.logger.LogAttrs(ctx, slog.LevelDebug,
							"verifying all retries have flushed",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
						)

						lt.resultWaitGroup.Wait()

						// read up to numNewTasks from retry slice again
						taskBufSize = readRetries(taskBuf[:numNewTasks:numNewTasks])
						if taskBufSize <= 0 {

							lt.logger.LogAttrs(ctx, slog.LevelInfo,
								"all retries flushed",
								slog.Int("preflush_num_tasks", preflushNumTasks),
								slog.Int("num_tasks", numTasks),
							)
							return nil
						}
					}
					taskBuf = taskBuf[:taskBufSize]

					// re-release any extra load slots we allocated beyond what really remains to do
					if numNewTasks > taskBufSize {
						lt.intervalTasksSema.Release(int64(numNewTasks - taskBufSize))
					}

					lt.resultWaitGroup.Add(taskBufSize + 1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
					lt.resultsChan <- taskResult{
						Meta: taskMeta{
							// IntervalID: intervalID, // not required unless in a debug context
							SampleSize: taskBufSize,
						},
					}

					meta.IntervalID = intervalID

					enqueueTasks()

					taskBuf = taskBuf[:0]

					numTasks += taskBufSize

					meta.Lag = 0

					// wait for next interval time to exist
					nextIntervalID := intervalID.Add(interval)
					realNow := time.Now()
					delay = nextIntervalID.Sub(realNow)
					if delay > 0 {
						time.Sleep(delay)
						intervalID = nextIntervalID

						if taskBufSize < numNewTasks {
							// just finished this iteration of retry enqueuing
							//
							// break to loop through retry drain context again
							break
						}

						continue
					}

					if delay < 0 {
						intervalID = realNow

						lag := -delay
						meta.Lag = lag

						lt.resultWaitGroup.Add(1)
						lt.resultsChan <- taskResult{
							Meta: taskMeta{
								// IntervalID: intervalID, // not required unless in a debug context
								Lag: lag,
							},
						}

					}

					if taskBufSize < numNewTasks {
						// just finished this iteration of retry enqueuing
						//
						// break to loop through retry drain context again
						break
					}
				}
			}
		}(lt.flushRetriesOnShutdown)
		if err != nil {
			*shutdownErrResp = err
		}

		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for running tasks to stop",
		)
		lt.resultWaitGroup.Wait()

		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping result handler routine",
		)

		// signal for result handler routines to stop
		close(lt.resultsChan)

		// signal for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping workers",
		)
		for i := 0; i < numSpawnedWorkers; i++ {
			close(lt.pauseChans[i])
		}

		// wait for result handler routines to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for result handler routines to stop",
		)
		wg.Wait()

		// wait for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for workers to stop",
		)
		lt.workerWaitGroup.Wait()

		lt.logger.LogAttrs(ctx, slog.LevelInfo,
			"loadtest stopped",
		)
	}()

	// getTaskSlotCount is the task emission back pressure
	// throttle that conveys the number of tasks that
	// are allowed to be un-finished for the performance
	// interval under normal circumstances
	getTaskSlotCount := func() int {
		return maxPendingTasks(numWorkers, numNewTasks)
	}

	// apply initial task buffer limits to the interval semaphore
	taskSlotCount := getTaskSlotCount()
	lt.intervalTasksSema.Release(int64(taskSlotCount))

	configCausesPause := func() bool {
		return meta.NumIntervalTasks <= 0 || numWorkers <= 0
	}

	var paused bool
	var pauseStart time.Time

	handleConfigUpdateAndPauseState := func(cu ConfigUpdate) error {
		for {
			var prepSemaErr error
			var recomputeInterTaskInterval, recomputeTaskSlots bool

			if cu.numWorkers.set {
				recomputeTaskSlots = true

				n := cu.numWorkers.val

				// prevent over committing on the maxWorkers count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxWorkers {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too high"),
						slog.String("remediation_hint", "increase the loadtest MaxWorkers setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxWorkers),
					)
					n = lt.maxWorkers
				}

				if n > numWorkers {

					// unpause workers
					for i := numWorkers; i < numSpawnedWorkers; i++ {
						lt.pauseChans[i] <- struct{}{}
					}

					// spawn new workers if needed
					for i := numSpawnedWorkers; i < n; i++ {
						lt.addWorker(ctx, i)
						numSpawnedWorkers++
					}
				} else if n < numWorkers {

					// pause workers if needed
					for i := numWorkers - 1; i >= n; i-- {
						lt.pauseChans[i] <- struct{}{}
					}
				}

				configChanges = append(configChanges,
					slog.Int("old_num_workers", numWorkers),
					slog.Int("new_num_workers", n),
				)
				numWorkers = n
			}

			if cu.numIntervalTasks.set {
				recomputeInterTaskInterval = true
				recomputeTaskSlots = true

				n := cu.numIntervalTasks.val

				// prevent over committing on the maxIntervalTasks count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxIntervalTasks {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too high"),
						slog.String("remediation_hint", "increase the loadtest MaxIntervalTasks setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxIntervalTasks),
					)
					n = lt.maxIntervalTasks
				}

				configChanges = append(configChanges,
					slog.Int("old_num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("new_num_interval_tasks", n),
				)
				numNewTasks = n
				meta.NumIntervalTasks = n
			}

			if cu.interval.set {
				recomputeInterTaskInterval = true

				n := cu.interval.val

				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: interval",
						slog.String("reason", "update tried to set interval too low"),
						slog.String("remediation_taken", "using min value"),
						slog.String("requested", n.String()),
						slog.String("min", time.Duration(0).String()),
					)
					n = 0
				}

				configChanges = append(configChanges,
					slog.String("old_interval", interval.String()),
					slog.String("new_interval", n.String()),
				)
				interval = n
			}

			// && clause: protects against divide by zero
			if recomputeInterTaskInterval && meta.NumIntervalTasks > 0 {
				interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
				updateEnqueueTasksStrategy()
			}

			if recomputeTaskSlots {
				if newTaskSlotCount := getTaskSlotCount(); newTaskSlotCount != taskSlotCount {

					if newTaskSlotCount > taskSlotCount {
						lt.intervalTasksSema.Release(int64(newTaskSlotCount - taskSlotCount))
					} else {
						prepSemaErr = lt.intervalTasksSema.Acquire(ctx, int64(taskSlotCount-newTaskSlotCount))
						if prepSemaErr != nil {
							lt.logger.LogAttrs(ctx, slog.LevelError,
								"loadtest config update: failed to pre-acquire load generation slots",
								slog.Any("error", prepSemaErr),
							)

							// not returning and error... yet
							// going to let config update log statement occur and then report the error present in prepSemaErr
						}
					}

					taskSlotCount = newTaskSlotCount
				}
			}

			if !cu.onStartup {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"loadtest config updated",
					configChanges...,
				)
			}
			configChanges = configChanges[:0]

			if prepSemaErr != nil {
				return prepSemaErr
			}

			// pause load generation if unable to schedule anything
			if configCausesPause() {

				if !paused {
					paused = true
					pauseStart = time.Now().UTC()

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"pausing load generation",
						slog.Int("num_interval_tasks", meta.NumIntervalTasks),
						slog.Int("num_workers", numWorkers),
						slog.String("paused_at", pauseStart.String()),
					)
				}

				// duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
				// that follows
				//
				// ref: https://go.dev/ref/spec#Select_statements
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				default:
				}
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				case cu = <-cfgUpdateChan:
					continue
				}
			}

			if paused {
				paused = false
				intervalID = time.Now()

				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"resuming load generation",
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
					slog.String("paused_at", pauseStart.String()),
					slog.String("resumed_at", intervalID.UTC().String()),
				)
			}

			return nil
		}
	}

	if configCausesPause() {
		if err := handleConfigUpdateAndPauseState(ConfigUpdate{onStartup: true}); err != nil {
			if err == errLoadtestContextDone {
				return nil
			}
			return err
		}
	}

	// start workers just before starting task scheduling
	for i := 0; i < numWorkers; i++ {
		lt.addWorker(ctx, i)
		numSpawnedWorkers++
	}

	// main task scheduling loop
	for {

		// Not duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
		// that follows because this is a ordered sequence where all selects in the sequence have non-blocking default cases.
		//
		// The odds of thread switching between these selects is minimal as is the impact of having to wait at most one more cycle
		// to short circuit and return.
		//
		// ref: https://go.dev/ref/spec#Select_statements
		select {
		case <-ctxDone:
			return nil
		default:
		}
		select {
		case cu := <-cfgUpdateChan:
			if err := handleConfigUpdateAndPauseState(cu); err != nil {
				if err == errLoadtestContextDone {
					return nil
				}
				return err
			}

			// re-loop
			continue
		default:
			// continue with load generation
		}

		taskBufSize := 0

		//
		// read up to numNewTasks from retry slice
		//

		// acquire load generation opportunity slots ( smooths bursts )
		//
		// do this early conditionally to allow retries to settle in the retry channel
		// so we can pick them up when enough buffer space has cleared
		//
		// thus we avoid a possible deadlock where the retry queue is full and the workers
		// all have failed tasks that wish to be retried
		if lt.intervalTasksSema.Acquire(ctx, int64(numNewTasks)) != nil {
			return nil
		}

		taskBufSize = readRetries(taskBuf[:numNewTasks:numNewTasks])

		taskBuf = taskBuf[:taskBufSize]

		if taskBufSize < numNewTasks {
			maxSize := numNewTasks - taskBufSize
			n := taskReader.ReadTasks(taskBuf[taskBufSize:numNewTasks:numNewTasks])
			if n < 0 || n > maxSize {
				panic(ErrBadReadTasksImpl)
			}
			if n == 0 {

				// iteration is technically done now
				// but there could be straggling retries
				// queued after this, those should continue
				// to be flushed if and only if maxTasks
				// has not been reached and if it is greater
				// than zero
				if taskBufSize == 0 {
					// return immediately if there is nothing
					// new to enqueue

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"stopping loadtest: ReadTasks did not load enough tasks",
						slog.Int("final_task_delta", 0),
					)

					return nil
				}

				lt.logger.LogAttrs(ctx, slog.LevelDebug,
					"scheduled: stopping loadtest: ReadTasks did not load enough tasks",
					slog.Int("retry_count", taskBufSize),
				)
			}

			taskBufSize += n
			taskBuf = taskBuf[:taskBufSize]
		}

		// acquire load generation opportunity slots ( smooths bursts )
		// if not done already
		//
		// but if we allocated too many in our retry prep phase then release the
		// difference
		if numNewTasks > taskBufSize {
			lt.intervalTasksSema.Release(int64(numNewTasks - taskBufSize))
		}

		lt.resultWaitGroup.Add(taskBufSize + 1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
		lt.resultsChan <- taskResult{
			Meta: taskMeta{
				// IntervalID: intervalID, // not required unless in a debug context
				SampleSize: taskBufSize,
			},
		}

		meta.IntervalID = intervalID

		enqueueTasks()

		if numNewTasks > taskBufSize {
			// must have hit the end of ReadTasks iterator
			// increase numTasks total by actual number queued
			// and stop traffic generation
			numTasks += taskBufSize
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"stopping loadtest: ReadTasks did not load enough tasks",
				slog.Int("final_task_delta", taskBufSize),
			)
			return nil
		}

		taskBuf = taskBuf[:0]

		numTasks += taskBufSize

		meta.Lag = 0

		// wait for next interval time to exist
		nextIntervalID := intervalID.Add(interval)
		realNow := time.Now()
		delay = nextIntervalID.Sub(realNow)
		if delay > 0 {
			time.Sleep(delay)
			intervalID = nextIntervalID
			continue
		}

		if delay < 0 {
			intervalID = realNow

			lag := -delay
			meta.Lag = lag

			lt.resultWaitGroup.Add(1)
			lt.resultsChan <- taskResult{
				Meta: taskMeta{
					// IntervalID: intervalID, // not required unless in a debug context
					Lag: lag,
				},
			}

		}
	}
}

func (lt *Loadtest) run_retriesEnabled_maxTasksNotGTZero_metricsDisabled(ctx context.Context, shutdownErrResp *error) error {

	cfgUpdateChan := lt.cfgUpdateChan
	defer close(cfgUpdateChan)

	lt.startTime = time.Now()

	lt.logger.LogAttrs(ctx, slog.LevelInfo,
		"starting loadtest",
		slog.Any("config", lt.loadtestConfigAsJson()),
	)

	numWorkers := lt.numWorkers
	numSpawnedWorkers := 0

	// numTasks is the total number of tasks
	// scheduled to run ( including retries )
	var numTasks int

	intervalID := time.Now()

	interval := lt.interval
	numNewTasks := lt.numIntervalTasks
	ctxDone := ctx.Done()
	taskReader := lt.taskReader
	configChanges := make([]slog.Attr, 0, 6)
	meta := taskMeta{
		NumIntervalTasks: lt.numIntervalTasks,
	}
	var interTaskInterval time.Duration
	if meta.NumIntervalTasks > 0 {
		interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
	}

	taskBuf := make([]Doer, 0, lt.maxIntervalTasks)

	var enqueueTasks func()
	var updateEnqueueTasksStrategy func()
	{
		floodStrategy := func() {
			for _, task := range taskBuf {
				lt.taskChan <- taskWithMeta{task, intervalID, meta}
			}
		}

		staggerStrategy := func() {
			lt.taskChan <- taskWithMeta{taskBuf[0], intervalID, meta}

			for _, task := range taskBuf[1:] {
				time.Sleep(interTaskInterval)
				lt.taskChan <- taskWithMeta{task, time.Now(), meta}
			}
		}

		updateEnqueueTasksStrategy = func() {
			if interTaskInterval <= skipInterTaskSchedulingThreshold {
				enqueueTasks = floodStrategy
			} else {
				enqueueTasks = staggerStrategy
			}
		}
	}
	updateEnqueueTasksStrategy()

	var delay time.Duration

	readRetries := func(p []Doer) int {
		// make sure you only fill up to len

		var i int
		for i < len(p) {
			select {
			case task := <-lt.retryTaskChan:
				p[i] = task
			default:
				return i
			}
			i++
		}

		return i
	}

	// stopping routine runs on return
	// flushing as much as possible
	defer func() {

		err := func(flushRetries bool) error {
			if !flushRetries {

				lt.logger.LogAttrs(ctx, slog.LevelDebug,
					"not waiting on retries to flush on shutdown",
					slog.String("reason", "retries disabled or flush retries on shutdown disabled"),
					slog.Int("num_tasks", numTasks),
				)

				return nil
			}

			if err := ctx.Err(); err != nil {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"not waiting on retries to flush on shutdown",
					slog.String("reason", "user stopped loadtest"),
					slog.Int("num_tasks", numTasks),
					slog.Any("error", err),
				)
				return nil
			}

			lt.logger.LogAttrs(ctx, slog.LevelDebug,
				"waiting on retries to flush",
				slog.Int("num_tasks", numTasks),
			)

			if meta.NumIntervalTasks <= 0 || numWorkers <= 0 {

				lt.logger.LogAttrs(ctx, slog.LevelError,
					"retry flushing could not be attempted",
					slog.Int("num_tasks", numTasks),
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
				)

				return ErrRetriesFailedToFlush
			}

			preflushNumTasks := numTasks

			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"shutting down: flushing retries",
				slog.Int("num_tasks", numTasks),
				slog.String("flush_retries_timeout", lt.flushRetriesTimeout.String()),
			)

			shutdownCtx, cancel := context.WithTimeout(context.Background(), lt.flushRetriesTimeout)
			defer cancel()

			intervalID = time.Now()
			taskBuf = taskBuf[:0]

			for {

				if err := shutdownCtx.Err(); err != nil {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"failed to flush all retries",
						slog.Int("preflush_num_tasks", preflushNumTasks),
						slog.Int("num_tasks", numTasks),
						slog.Any("error", err),
					)

					return ErrRetriesFailedToFlush
				}

				lt.resultWaitGroup.Wait()

				for {

					if err := shutdownCtx.Err(); err != nil {
						lt.logger.LogAttrs(ctx, slog.LevelError,
							"failed to flush all retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
							slog.Any("error", err),
						)

						return ErrRetriesFailedToFlush
					}

					select {
					case <-ctxDone:
						lt.logger.LogAttrs(ctx, slog.LevelWarn,
							"user stopped loadtest while attempting to flush retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
						)
						return nil
					default:
						// continue with load generating retries
					}

					// acquire load generation opportunity slots ( smooths bursts )
					//
					// in the shutdown retry flow we always want to acquire before reading retries
					// to avoid a deadlock edge case of the retry queue being full, all workers tasks failed and need to be retried
					if err := lt.intervalTasksSema.Acquire(shutdownCtx, int64(numNewTasks)); err != nil {
						lt.logger.LogAttrs(ctx, slog.LevelError,
							"failed to flush all retries",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
							slog.Any("error", err),
							slog.String("reason", "shutdown timeout likely reached while waiting for semaphore acquisition"),
						)
						return ErrRetriesFailedToFlush
					}

					// read up to numNewTasks from retry slice
					taskBufSize := readRetries(taskBuf[:numNewTasks:numNewTasks])
					if taskBufSize <= 0 {
						// wait for any pending tasks to flush and try read again

						lt.logger.LogAttrs(ctx, slog.LevelDebug,
							"verifying all retries have flushed",
							slog.Int("preflush_num_tasks", preflushNumTasks),
							slog.Int("num_tasks", numTasks),
						)

						lt.resultWaitGroup.Wait()

						// read up to numNewTasks from retry slice again
						taskBufSize = readRetries(taskBuf[:numNewTasks:numNewTasks])
						if taskBufSize <= 0 {

							lt.logger.LogAttrs(ctx, slog.LevelInfo,
								"all retries flushed",
								slog.Int("preflush_num_tasks", preflushNumTasks),
								slog.Int("num_tasks", numTasks),
							)
							return nil
						}
					}
					taskBuf = taskBuf[:taskBufSize]

					// re-release any extra load slots we allocated beyond what really remains to do
					if numNewTasks > taskBufSize {
						lt.intervalTasksSema.Release(int64(numNewTasks - taskBufSize))
					}

					lt.resultWaitGroup.Add(taskBufSize + 1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
					lt.resultsChan <- taskResult{
						Meta: taskMeta{
							// IntervalID: intervalID, // not required unless in a debug context
							SampleSize: taskBufSize,
						},
					}

					meta.IntervalID = intervalID

					enqueueTasks()

					taskBuf = taskBuf[:0]

					numTasks += taskBufSize

					// wait for next interval time to exist
					nextIntervalID := intervalID.Add(interval)
					realNow := time.Now()
					delay = nextIntervalID.Sub(realNow)
					if delay > 0 {
						time.Sleep(delay)
						intervalID = nextIntervalID

						if taskBufSize < numNewTasks {
							// just finished this iteration of retry enqueuing
							//
							// break to loop through retry drain context again
							break
						}

						continue
					}

					if delay < 0 {
						intervalID = realNow

					}

					if taskBufSize < numNewTasks {
						// just finished this iteration of retry enqueuing
						//
						// break to loop through retry drain context again
						break
					}
				}
			}
		}(lt.flushRetriesOnShutdown)
		if err != nil {
			*shutdownErrResp = err
		}

		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for running tasks to stop",
		)
		lt.resultWaitGroup.Wait()

		// signal for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping workers",
		)
		for i := 0; i < numSpawnedWorkers; i++ {
			close(lt.pauseChans[i])
		}

		// wait for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for workers to stop",
		)
		lt.workerWaitGroup.Wait()

		lt.logger.LogAttrs(ctx, slog.LevelInfo,
			"loadtest stopped",
		)
	}()

	// getTaskSlotCount is the task emission back pressure
	// throttle that conveys the number of tasks that
	// are allowed to be un-finished for the performance
	// interval under normal circumstances
	getTaskSlotCount := func() int {
		return maxPendingTasks(numWorkers, numNewTasks)
	}

	// apply initial task buffer limits to the interval semaphore
	taskSlotCount := getTaskSlotCount()
	lt.intervalTasksSema.Release(int64(taskSlotCount))

	configCausesPause := func() bool {
		return meta.NumIntervalTasks <= 0 || numWorkers <= 0
	}

	var paused bool
	var pauseStart time.Time

	handleConfigUpdateAndPauseState := func(cu ConfigUpdate) error {
		for {
			var prepSemaErr error
			var recomputeInterTaskInterval, recomputeTaskSlots bool

			if cu.numWorkers.set {
				recomputeTaskSlots = true

				n := cu.numWorkers.val

				// prevent over committing on the maxWorkers count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxWorkers {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too high"),
						slog.String("remediation_hint", "increase the loadtest MaxWorkers setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxWorkers),
					)
					n = lt.maxWorkers
				}

				if n > numWorkers {

					// unpause workers
					for i := numWorkers; i < numSpawnedWorkers; i++ {
						lt.pauseChans[i] <- struct{}{}
					}

					// spawn new workers if needed
					for i := numSpawnedWorkers; i < n; i++ {
						lt.addWorker(ctx, i)
						numSpawnedWorkers++
					}
				} else if n < numWorkers {

					// pause workers if needed
					for i := numWorkers - 1; i >= n; i-- {
						lt.pauseChans[i] <- struct{}{}
					}
				}

				configChanges = append(configChanges,
					slog.Int("old_num_workers", numWorkers),
					slog.Int("new_num_workers", n),
				)
				numWorkers = n
			}

			if cu.numIntervalTasks.set {
				recomputeInterTaskInterval = true
				recomputeTaskSlots = true

				n := cu.numIntervalTasks.val

				// prevent over committing on the maxIntervalTasks count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxIntervalTasks {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too high"),
						slog.String("remediation_hint", "increase the loadtest MaxIntervalTasks setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxIntervalTasks),
					)
					n = lt.maxIntervalTasks
				}

				configChanges = append(configChanges,
					slog.Int("old_num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("new_num_interval_tasks", n),
				)
				numNewTasks = n
				meta.NumIntervalTasks = n
			}

			if cu.interval.set {
				recomputeInterTaskInterval = true

				n := cu.interval.val

				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: interval",
						slog.String("reason", "update tried to set interval too low"),
						slog.String("remediation_taken", "using min value"),
						slog.String("requested", n.String()),
						slog.String("min", time.Duration(0).String()),
					)
					n = 0
				}

				configChanges = append(configChanges,
					slog.String("old_interval", interval.String()),
					slog.String("new_interval", n.String()),
				)
				interval = n
			}

			// && clause: protects against divide by zero
			if recomputeInterTaskInterval && meta.NumIntervalTasks > 0 {
				interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
				updateEnqueueTasksStrategy()
			}

			if recomputeTaskSlots {
				if newTaskSlotCount := getTaskSlotCount(); newTaskSlotCount != taskSlotCount {

					if newTaskSlotCount > taskSlotCount {
						lt.intervalTasksSema.Release(int64(newTaskSlotCount - taskSlotCount))
					} else {
						prepSemaErr = lt.intervalTasksSema.Acquire(ctx, int64(taskSlotCount-newTaskSlotCount))
						if prepSemaErr != nil {
							lt.logger.LogAttrs(ctx, slog.LevelError,
								"loadtest config update: failed to pre-acquire load generation slots",
								slog.Any("error", prepSemaErr),
							)

							// not returning and error... yet
							// going to let config update log statement occur and then report the error present in prepSemaErr
						}
					}

					taskSlotCount = newTaskSlotCount
				}
			}

			if !cu.onStartup {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"loadtest config updated",
					configChanges...,
				)
			}
			configChanges = configChanges[:0]

			if prepSemaErr != nil {
				return prepSemaErr
			}

			// pause load generation if unable to schedule anything
			if configCausesPause() {

				if !paused {
					paused = true
					pauseStart = time.Now().UTC()

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"pausing load generation",
						slog.Int("num_interval_tasks", meta.NumIntervalTasks),
						slog.Int("num_workers", numWorkers),
						slog.String("paused_at", pauseStart.String()),
					)
				}

				// duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
				// that follows
				//
				// ref: https://go.dev/ref/spec#Select_statements
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				default:
				}
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				case cu = <-cfgUpdateChan:
					continue
				}
			}

			if paused {
				paused = false
				intervalID = time.Now()

				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"resuming load generation",
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
					slog.String("paused_at", pauseStart.String()),
					slog.String("resumed_at", intervalID.UTC().String()),
				)
			}

			return nil
		}
	}

	if configCausesPause() {
		if err := handleConfigUpdateAndPauseState(ConfigUpdate{onStartup: true}); err != nil {
			if err == errLoadtestContextDone {
				return nil
			}
			return err
		}
	}

	// start workers just before starting task scheduling
	for i := 0; i < numWorkers; i++ {
		lt.addWorker(ctx, i)
		numSpawnedWorkers++
	}

	// main task scheduling loop
	for {

		// Not duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
		// that follows because this is a ordered sequence where all selects in the sequence have non-blocking default cases.
		//
		// The odds of thread switching between these selects is minimal as is the impact of having to wait at most one more cycle
		// to short circuit and return.
		//
		// ref: https://go.dev/ref/spec#Select_statements
		select {
		case <-ctxDone:
			return nil
		default:
		}
		select {
		case cu := <-cfgUpdateChan:
			if err := handleConfigUpdateAndPauseState(cu); err != nil {
				if err == errLoadtestContextDone {
					return nil
				}
				return err
			}

			// re-loop
			continue
		default:
			// continue with load generation
		}

		taskBufSize := 0

		//
		// read up to numNewTasks from retry slice
		//

		// acquire load generation opportunity slots ( smooths bursts )
		//
		// do this early conditionally to allow retries to settle in the retry channel
		// so we can pick them up when enough buffer space has cleared
		//
		// thus we avoid a possible deadlock where the retry queue is full and the workers
		// all have failed tasks that wish to be retried
		if lt.intervalTasksSema.Acquire(ctx, int64(numNewTasks)) != nil {
			return nil
		}

		taskBufSize = readRetries(taskBuf[:numNewTasks:numNewTasks])

		taskBuf = taskBuf[:taskBufSize]

		if taskBufSize < numNewTasks {
			maxSize := numNewTasks - taskBufSize
			n := taskReader.ReadTasks(taskBuf[taskBufSize:numNewTasks:numNewTasks])
			if n < 0 || n > maxSize {
				panic(ErrBadReadTasksImpl)
			}
			if n == 0 {

				// iteration is technically done now
				// but there could be straggling retries
				// queued after this, those should continue
				// to be flushed if and only if maxTasks
				// has not been reached and if it is greater
				// than zero
				if taskBufSize == 0 {
					// return immediately if there is nothing
					// new to enqueue

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"stopping loadtest: ReadTasks did not load enough tasks",
						slog.Int("final_task_delta", 0),
					)

					return nil
				}

				lt.logger.LogAttrs(ctx, slog.LevelDebug,
					"scheduled: stopping loadtest: ReadTasks did not load enough tasks",
					slog.Int("retry_count", taskBufSize),
				)
			}

			taskBufSize += n
			taskBuf = taskBuf[:taskBufSize]
		}

		// acquire load generation opportunity slots ( smooths bursts )
		// if not done already
		//
		// but if we allocated too many in our retry prep phase then release the
		// difference
		if numNewTasks > taskBufSize {
			lt.intervalTasksSema.Release(int64(numNewTasks - taskBufSize))
		}

		lt.resultWaitGroup.Add(taskBufSize + 1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
		lt.resultsChan <- taskResult{
			Meta: taskMeta{
				// IntervalID: intervalID, // not required unless in a debug context
				SampleSize: taskBufSize,
			},
		}

		meta.IntervalID = intervalID

		enqueueTasks()

		if numNewTasks > taskBufSize {
			// must have hit the end of ReadTasks iterator
			// increase numTasks total by actual number queued
			// and stop traffic generation
			numTasks += taskBufSize
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"stopping loadtest: ReadTasks did not load enough tasks",
				slog.Int("final_task_delta", taskBufSize),
			)
			return nil
		}

		taskBuf = taskBuf[:0]

		numTasks += taskBufSize

		// wait for next interval time to exist
		nextIntervalID := intervalID.Add(interval)
		realNow := time.Now()
		delay = nextIntervalID.Sub(realNow)
		if delay > 0 {
			time.Sleep(delay)
			intervalID = nextIntervalID
			continue
		}

		if delay < 0 {
			intervalID = realNow

		}
	}
}

func (lt *Loadtest) run_retriesDisabled_maxTasksGTZero_metricsEnabled(ctx context.Context, _ *error) error {

	cfgUpdateChan := lt.cfgUpdateChan
	defer close(cfgUpdateChan)

	lt.startTime = time.Now()

	{

		csvFile, err := os.Create(lt.csvData.outputFilename)
		if err != nil {
			return fmt.Errorf("failed to open output csv metrics file for writing: %w", err)
		}
		defer lt.writeOutputCsvFooterAndClose(csvFile)

		lt.csvData.writeErr = lt.writeOutputCsvConfigComment(csvFile)

		if lt.csvData.writeErr == nil {

			lt.csvData.writer = csv.NewWriter(csvFile)

			lt.csvData.writeErr = lt.writeOutputCsvHeaders()
		}
	}

	lt.logger.LogAttrs(ctx, slog.LevelInfo,
		"starting loadtest",
		slog.Any("config", lt.loadtestConfigAsJson()),
	)

	var wg sync.WaitGroup

	wg.Go(lt.resultsHandler)

	numWorkers := lt.numWorkers
	numSpawnedWorkers := 0

	// numTasks is the total number of tasks
	// scheduled to run ( including retries )
	var numTasks int

	intervalID := time.Now()

	maxTasks := lt.maxTasks

	interval := lt.interval
	numNewTasks := lt.numIntervalTasks
	ctxDone := ctx.Done()
	taskReader := lt.taskReader
	configChanges := make([]slog.Attr, 0, 6)
	meta := taskMeta{
		NumIntervalTasks: lt.numIntervalTasks,
	}
	var interTaskInterval time.Duration
	if meta.NumIntervalTasks > 0 {
		interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
	}

	taskBuf := make([]Doer, 0, lt.maxIntervalTasks)

	var enqueueTasks func()
	var updateEnqueueTasksStrategy func()
	{
		floodStrategy := func() {
			for _, task := range taskBuf {
				lt.taskChan <- taskWithMeta{task, intervalID, meta}
			}
		}

		staggerStrategy := func() {
			lt.taskChan <- taskWithMeta{taskBuf[0], intervalID, meta}

			for _, task := range taskBuf[1:] {
				time.Sleep(interTaskInterval)
				lt.taskChan <- taskWithMeta{task, time.Now(), meta}
			}
		}

		updateEnqueueTasksStrategy = func() {
			if interTaskInterval <= skipInterTaskSchedulingThreshold {
				enqueueTasks = floodStrategy
			} else {
				enqueueTasks = staggerStrategy
			}
		}
	}
	updateEnqueueTasksStrategy()

	var delay time.Duration

	// stopping routine runs on return
	// flushing as much as possible
	defer func() {

		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for running tasks to stop",
		)
		lt.resultWaitGroup.Wait()

		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping result handler routine",
		)

		// signal for result handler routines to stop
		close(lt.resultsChan)

		// signal for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping workers",
		)
		for i := 0; i < numSpawnedWorkers; i++ {
			close(lt.pauseChans[i])
		}

		// wait for result handler routines to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for result handler routines to stop",
		)
		wg.Wait()

		// wait for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for workers to stop",
		)
		lt.workerWaitGroup.Wait()

		lt.logger.LogAttrs(ctx, slog.LevelInfo,
			"loadtest stopped",
		)
	}()

	// getTaskSlotCount is the task emission back pressure
	// throttle that conveys the number of tasks that
	// are allowed to be un-finished for the performance
	// interval under normal circumstances
	getTaskSlotCount := func() int {
		return maxPendingTasks(numWorkers, numNewTasks)
	}

	// apply initial task buffer limits to the interval semaphore
	taskSlotCount := getTaskSlotCount()
	lt.intervalTasksSema.Release(int64(taskSlotCount))

	configCausesPause := func() bool {
		return meta.NumIntervalTasks <= 0 || numWorkers <= 0
	}

	var paused bool
	var pauseStart time.Time

	handleConfigUpdateAndPauseState := func(cu ConfigUpdate) error {
		for {
			var prepSemaErr error
			var recomputeInterTaskInterval, recomputeTaskSlots bool

			if cu.numWorkers.set {
				recomputeTaskSlots = true

				n := cu.numWorkers.val

				// prevent over committing on the maxWorkers count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxWorkers {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too high"),
						slog.String("remediation_hint", "increase the loadtest MaxWorkers setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxWorkers),
					)
					n = lt.maxWorkers
				}

				if n > numWorkers {

					// unpause workers
					for i := numWorkers; i < numSpawnedWorkers; i++ {
						lt.pauseChans[i] <- struct{}{}
					}

					// spawn new workers if needed
					for i := numSpawnedWorkers; i < n; i++ {
						lt.addWorker(ctx, i)
						numSpawnedWorkers++
					}
				} else if n < numWorkers {

					// pause workers if needed
					for i := numWorkers - 1; i >= n; i-- {
						lt.pauseChans[i] <- struct{}{}
					}
				}

				configChanges = append(configChanges,
					slog.Int("old_num_workers", numWorkers),
					slog.Int("new_num_workers", n),
				)
				numWorkers = n
			}

			if cu.numIntervalTasks.set {
				recomputeInterTaskInterval = true
				recomputeTaskSlots = true

				n := cu.numIntervalTasks.val

				// prevent over committing on the maxIntervalTasks count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxIntervalTasks {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too high"),
						slog.String("remediation_hint", "increase the loadtest MaxIntervalTasks setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxIntervalTasks),
					)
					n = lt.maxIntervalTasks
				}

				configChanges = append(configChanges,
					slog.Int("old_num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("new_num_interval_tasks", n),
				)
				numNewTasks = n
				meta.NumIntervalTasks = n
			}

			if cu.interval.set {
				recomputeInterTaskInterval = true

				n := cu.interval.val

				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: interval",
						slog.String("reason", "update tried to set interval too low"),
						slog.String("remediation_taken", "using min value"),
						slog.String("requested", n.String()),
						slog.String("min", time.Duration(0).String()),
					)
					n = 0
				}

				configChanges = append(configChanges,
					slog.String("old_interval", interval.String()),
					slog.String("new_interval", n.String()),
				)
				interval = n
			}

			// && clause: protects against divide by zero
			if recomputeInterTaskInterval && meta.NumIntervalTasks > 0 {
				interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
				updateEnqueueTasksStrategy()
			}

			if recomputeTaskSlots {
				if newTaskSlotCount := getTaskSlotCount(); newTaskSlotCount != taskSlotCount {

					if newTaskSlotCount > taskSlotCount {
						lt.intervalTasksSema.Release(int64(newTaskSlotCount - taskSlotCount))
					} else {
						prepSemaErr = lt.intervalTasksSema.Acquire(ctx, int64(taskSlotCount-newTaskSlotCount))
						if prepSemaErr != nil {
							lt.logger.LogAttrs(ctx, slog.LevelError,
								"loadtest config update: failed to pre-acquire load generation slots",
								slog.Any("error", prepSemaErr),
							)

							// not returning and error... yet
							// going to let config update log statement occur and then report the error present in prepSemaErr
						}
					}

					taskSlotCount = newTaskSlotCount
				}
			}

			if !cu.onStartup {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"loadtest config updated",
					configChanges...,
				)
			}
			configChanges = configChanges[:0]

			if prepSemaErr != nil {
				return prepSemaErr
			}

			// pause load generation if unable to schedule anything
			if configCausesPause() {

				if !paused {
					paused = true
					pauseStart = time.Now().UTC()

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"pausing load generation",
						slog.Int("num_interval_tasks", meta.NumIntervalTasks),
						slog.Int("num_workers", numWorkers),
						slog.String("paused_at", pauseStart.String()),
					)
				}

				// duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
				// that follows
				//
				// ref: https://go.dev/ref/spec#Select_statements
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				default:
				}
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				case cu = <-cfgUpdateChan:
					continue
				}
			}

			if paused {
				paused = false
				intervalID = time.Now()

				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"resuming load generation",
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
					slog.String("paused_at", pauseStart.String()),
					slog.String("resumed_at", intervalID.UTC().String()),
				)
			}

			return nil
		}
	}

	if configCausesPause() {
		if err := handleConfigUpdateAndPauseState(ConfigUpdate{onStartup: true}); err != nil {
			if err == errLoadtestContextDone {
				return nil
			}
			return err
		}
	}

	// start workers just before starting task scheduling
	for i := 0; i < numWorkers; i++ {
		lt.addWorker(ctx, i)
		numSpawnedWorkers++
	}

	// main task scheduling loop
	for {
		if numTasks >= maxTasks {
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"loadtest finished: max task count reached",
				slog.Int("max_tasks", maxTasks),
			)
			return nil
		}

		numNewTasks = maxTasks - numTasks
		if numNewTasks > meta.NumIntervalTasks {
			numNewTasks = meta.NumIntervalTasks
		}

		// Not duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
		// that follows because this is a ordered sequence where all selects in the sequence have non-blocking default cases.
		//
		// The odds of thread switching between these selects is minimal as is the impact of having to wait at most one more cycle
		// to short circuit and return.
		//
		// ref: https://go.dev/ref/spec#Select_statements
		select {
		case <-ctxDone:
			return nil
		default:
		}
		select {
		case cu := <-cfgUpdateChan:
			if err := handleConfigUpdateAndPauseState(cu); err != nil {
				if err == errLoadtestContextDone {
					return nil
				}
				return err
			}

			// re-loop
			continue
		default:
			// continue with load generation
		}

		taskBufSize := 0
		taskBuf = taskBuf[:taskBufSize]

		if taskBufSize < numNewTasks {
			maxSize := numNewTasks - taskBufSize
			n := taskReader.ReadTasks(taskBuf[taskBufSize:numNewTasks:numNewTasks])
			if n < 0 || n > maxSize {
				panic(ErrBadReadTasksImpl)
			}
			if n == 0 {

				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"stopping loadtest: ReadTasks did not load enough tasks",
					slog.Int("final_task_delta", 0),
				)

				return nil
			}

			taskBufSize += n
			taskBuf = taskBuf[:taskBufSize]
		}

		// acquire load generation opportunity slots ( smooths bursts )
		// if not done already
		if lt.intervalTasksSema.Acquire(ctx, int64(taskBufSize)) != nil {
			return nil
		}

		lt.resultWaitGroup.Add(taskBufSize + 1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
		lt.resultsChan <- taskResult{
			Meta: taskMeta{
				// IntervalID: intervalID, // not required unless in a debug context
				SampleSize: taskBufSize,
			},
		}

		meta.IntervalID = intervalID

		enqueueTasks()

		if numNewTasks > taskBufSize {
			// must have hit the end of ReadTasks iterator
			// increase numTasks total by actual number queued
			// and stop traffic generation
			// numTasks += taskBufSize // this line only has an effect except in a retries enabled context
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"stopping loadtest: ReadTasks did not load enough tasks",
				slog.Int("final_task_delta", taskBufSize),
			)
			return nil
		}

		taskBuf = taskBuf[:0]

		numTasks += taskBufSize

		meta.Lag = 0

		// wait for next interval time to exist
		nextIntervalID := intervalID.Add(interval)
		realNow := time.Now()
		delay = nextIntervalID.Sub(realNow)
		if delay > 0 {
			time.Sleep(delay)
			intervalID = nextIntervalID
			continue
		}

		if delay < 0 {
			intervalID = realNow

			lag := -delay
			meta.Lag = lag

			lt.resultWaitGroup.Add(1)
			lt.resultsChan <- taskResult{
				Meta: taskMeta{
					// IntervalID: intervalID, // not required unless in a debug context
					Lag: lag,
				},
			}

		}
	}
}

func (lt *Loadtest) run_retriesDisabled_maxTasksGTZero_metricsDisabled(ctx context.Context, _ *error) error {

	cfgUpdateChan := lt.cfgUpdateChan
	defer close(cfgUpdateChan)

	lt.startTime = time.Now()

	lt.logger.LogAttrs(ctx, slog.LevelInfo,
		"starting loadtest",
		slog.Any("config", lt.loadtestConfigAsJson()),
	)

	numWorkers := lt.numWorkers
	numSpawnedWorkers := 0

	// numTasks is the total number of tasks
	// scheduled to run ( including retries )
	var numTasks int

	intervalID := time.Now()

	maxTasks := lt.maxTasks

	interval := lt.interval
	numNewTasks := lt.numIntervalTasks
	ctxDone := ctx.Done()
	taskReader := lt.taskReader
	configChanges := make([]slog.Attr, 0, 6)
	meta := taskMeta{
		NumIntervalTasks: lt.numIntervalTasks,
	}
	var interTaskInterval time.Duration
	if meta.NumIntervalTasks > 0 {
		interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
	}

	taskBuf := make([]Doer, 0, lt.maxIntervalTasks)

	var enqueueTasks func()
	var updateEnqueueTasksStrategy func()
	{
		floodStrategy := func() {
			for _, task := range taskBuf {
				lt.taskChan <- taskWithMeta{task, intervalID, meta}
			}
		}

		staggerStrategy := func() {
			lt.taskChan <- taskWithMeta{taskBuf[0], intervalID, meta}

			for _, task := range taskBuf[1:] {
				time.Sleep(interTaskInterval)
				lt.taskChan <- taskWithMeta{task, time.Now(), meta}
			}
		}

		updateEnqueueTasksStrategy = func() {
			if interTaskInterval <= skipInterTaskSchedulingThreshold {
				enqueueTasks = floodStrategy
			} else {
				enqueueTasks = staggerStrategy
			}
		}
	}
	updateEnqueueTasksStrategy()

	var delay time.Duration

	// stopping routine runs on return
	// flushing as much as possible
	defer func() {

		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for running tasks to stop",
		)
		lt.resultWaitGroup.Wait()

		// signal for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping workers",
		)
		for i := 0; i < numSpawnedWorkers; i++ {
			close(lt.pauseChans[i])
		}

		// wait for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for workers to stop",
		)
		lt.workerWaitGroup.Wait()

		lt.logger.LogAttrs(ctx, slog.LevelInfo,
			"loadtest stopped",
		)
	}()

	// getTaskSlotCount is the task emission back pressure
	// throttle that conveys the number of tasks that
	// are allowed to be un-finished for the performance
	// interval under normal circumstances
	getTaskSlotCount := func() int {
		return maxPendingTasks(numWorkers, numNewTasks)
	}

	// apply initial task buffer limits to the interval semaphore
	taskSlotCount := getTaskSlotCount()
	lt.intervalTasksSema.Release(int64(taskSlotCount))

	configCausesPause := func() bool {
		return meta.NumIntervalTasks <= 0 || numWorkers <= 0
	}

	var paused bool
	var pauseStart time.Time

	handleConfigUpdateAndPauseState := func(cu ConfigUpdate) error {
		for {
			var prepSemaErr error
			var recomputeInterTaskInterval, recomputeTaskSlots bool

			if cu.numWorkers.set {
				recomputeTaskSlots = true

				n := cu.numWorkers.val

				// prevent over committing on the maxWorkers count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxWorkers {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too high"),
						slog.String("remediation_hint", "increase the loadtest MaxWorkers setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxWorkers),
					)
					n = lt.maxWorkers
				}

				if n > numWorkers {

					// unpause workers
					for i := numWorkers; i < numSpawnedWorkers; i++ {
						lt.pauseChans[i] <- struct{}{}
					}

					// spawn new workers if needed
					for i := numSpawnedWorkers; i < n; i++ {
						lt.addWorker(ctx, i)
						numSpawnedWorkers++
					}
				} else if n < numWorkers {

					// pause workers if needed
					for i := numWorkers - 1; i >= n; i-- {
						lt.pauseChans[i] <- struct{}{}
					}
				}

				configChanges = append(configChanges,
					slog.Int("old_num_workers", numWorkers),
					slog.Int("new_num_workers", n),
				)
				numWorkers = n
			}

			if cu.numIntervalTasks.set {
				recomputeInterTaskInterval = true
				recomputeTaskSlots = true

				n := cu.numIntervalTasks.val

				// prevent over committing on the maxIntervalTasks count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxIntervalTasks {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too high"),
						slog.String("remediation_hint", "increase the loadtest MaxIntervalTasks setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxIntervalTasks),
					)
					n = lt.maxIntervalTasks
				}

				configChanges = append(configChanges,
					slog.Int("old_num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("new_num_interval_tasks", n),
				)
				numNewTasks = n
				meta.NumIntervalTasks = n
			}

			if cu.interval.set {
				recomputeInterTaskInterval = true

				n := cu.interval.val

				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: interval",
						slog.String("reason", "update tried to set interval too low"),
						slog.String("remediation_taken", "using min value"),
						slog.String("requested", n.String()),
						slog.String("min", time.Duration(0).String()),
					)
					n = 0
				}

				configChanges = append(configChanges,
					slog.String("old_interval", interval.String()),
					slog.String("new_interval", n.String()),
				)
				interval = n
			}

			// && clause: protects against divide by zero
			if recomputeInterTaskInterval && meta.NumIntervalTasks > 0 {
				interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
				updateEnqueueTasksStrategy()
			}

			if recomputeTaskSlots {
				if newTaskSlotCount := getTaskSlotCount(); newTaskSlotCount != taskSlotCount {

					if newTaskSlotCount > taskSlotCount {
						lt.intervalTasksSema.Release(int64(newTaskSlotCount - taskSlotCount))
					} else {
						prepSemaErr = lt.intervalTasksSema.Acquire(ctx, int64(taskSlotCount-newTaskSlotCount))
						if prepSemaErr != nil {
							lt.logger.LogAttrs(ctx, slog.LevelError,
								"loadtest config update: failed to pre-acquire load generation slots",
								slog.Any("error", prepSemaErr),
							)

							// not returning and error... yet
							// going to let config update log statement occur and then report the error present in prepSemaErr
						}
					}

					taskSlotCount = newTaskSlotCount
				}
			}

			if !cu.onStartup {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"loadtest config updated",
					configChanges...,
				)
			}
			configChanges = configChanges[:0]

			if prepSemaErr != nil {
				return prepSemaErr
			}

			// pause load generation if unable to schedule anything
			if configCausesPause() {

				if !paused {
					paused = true
					pauseStart = time.Now().UTC()

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"pausing load generation",
						slog.Int("num_interval_tasks", meta.NumIntervalTasks),
						slog.Int("num_workers", numWorkers),
						slog.String("paused_at", pauseStart.String()),
					)
				}

				// duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
				// that follows
				//
				// ref: https://go.dev/ref/spec#Select_statements
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				default:
				}
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				case cu = <-cfgUpdateChan:
					continue
				}
			}

			if paused {
				paused = false
				intervalID = time.Now()

				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"resuming load generation",
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
					slog.String("paused_at", pauseStart.String()),
					slog.String("resumed_at", intervalID.UTC().String()),
				)
			}

			return nil
		}
	}

	if configCausesPause() {
		if err := handleConfigUpdateAndPauseState(ConfigUpdate{onStartup: true}); err != nil {
			if err == errLoadtestContextDone {
				return nil
			}
			return err
		}
	}

	// start workers just before starting task scheduling
	for i := 0; i < numWorkers; i++ {
		lt.addWorker(ctx, i)
		numSpawnedWorkers++
	}

	// main task scheduling loop
	for {
		if numTasks >= maxTasks {
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"loadtest finished: max task count reached",
				slog.Int("max_tasks", maxTasks),
			)
			return nil
		}

		numNewTasks = maxTasks - numTasks
		if numNewTasks > meta.NumIntervalTasks {
			numNewTasks = meta.NumIntervalTasks
		}

		// Not duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
		// that follows because this is a ordered sequence where all selects in the sequence have non-blocking default cases.
		//
		// The odds of thread switching between these selects is minimal as is the impact of having to wait at most one more cycle
		// to short circuit and return.
		//
		// ref: https://go.dev/ref/spec#Select_statements
		select {
		case <-ctxDone:
			return nil
		default:
		}
		select {
		case cu := <-cfgUpdateChan:
			if err := handleConfigUpdateAndPauseState(cu); err != nil {
				if err == errLoadtestContextDone {
					return nil
				}
				return err
			}

			// re-loop
			continue
		default:
			// continue with load generation
		}

		taskBufSize := 0
		taskBuf = taskBuf[:taskBufSize]

		if taskBufSize < numNewTasks {
			maxSize := numNewTasks - taskBufSize
			n := taskReader.ReadTasks(taskBuf[taskBufSize:numNewTasks:numNewTasks])
			if n < 0 || n > maxSize {
				panic(ErrBadReadTasksImpl)
			}
			if n == 0 {

				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"stopping loadtest: ReadTasks did not load enough tasks",
					slog.Int("final_task_delta", 0),
				)

				return nil
			}

			taskBufSize += n
			taskBuf = taskBuf[:taskBufSize]
		}

		// acquire load generation opportunity slots ( smooths bursts )
		// if not done already
		if lt.intervalTasksSema.Acquire(ctx, int64(taskBufSize)) != nil {
			return nil
		}

		lt.resultWaitGroup.Add(taskBufSize + 1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
		lt.resultsChan <- taskResult{
			Meta: taskMeta{
				// IntervalID: intervalID, // not required unless in a debug context
				SampleSize: taskBufSize,
			},
		}

		meta.IntervalID = intervalID

		enqueueTasks()

		if numNewTasks > taskBufSize {
			// must have hit the end of ReadTasks iterator
			// increase numTasks total by actual number queued
			// and stop traffic generation
			// numTasks += taskBufSize // this line only has an effect except in a retries enabled context
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"stopping loadtest: ReadTasks did not load enough tasks",
				slog.Int("final_task_delta", taskBufSize),
			)
			return nil
		}

		taskBuf = taskBuf[:0]

		numTasks += taskBufSize

		// wait for next interval time to exist
		nextIntervalID := intervalID.Add(interval)
		realNow := time.Now()
		delay = nextIntervalID.Sub(realNow)
		if delay > 0 {
			time.Sleep(delay)
			intervalID = nextIntervalID
			continue
		}

		if delay < 0 {
			intervalID = realNow

		}
	}
}

func (lt *Loadtest) run_retriesDisabled_maxTasksNotGTZero_metricsEnabled(ctx context.Context, _ *error) error {

	cfgUpdateChan := lt.cfgUpdateChan
	defer close(cfgUpdateChan)

	lt.startTime = time.Now()

	{

		csvFile, err := os.Create(lt.csvData.outputFilename)
		if err != nil {
			return fmt.Errorf("failed to open output csv metrics file for writing: %w", err)
		}
		defer lt.writeOutputCsvFooterAndClose(csvFile)

		lt.csvData.writeErr = lt.writeOutputCsvConfigComment(csvFile)

		if lt.csvData.writeErr == nil {

			lt.csvData.writer = csv.NewWriter(csvFile)

			lt.csvData.writeErr = lt.writeOutputCsvHeaders()
		}
	}

	lt.logger.LogAttrs(ctx, slog.LevelInfo,
		"starting loadtest",
		slog.Any("config", lt.loadtestConfigAsJson()),
	)

	var wg sync.WaitGroup

	wg.Go(lt.resultsHandler)

	numWorkers := lt.numWorkers
	numSpawnedWorkers := 0

	// numTasks is the total number of tasks
	// scheduled to run ( including retries )
	var numTasks int

	intervalID := time.Now()

	interval := lt.interval
	numNewTasks := lt.numIntervalTasks
	ctxDone := ctx.Done()
	taskReader := lt.taskReader
	configChanges := make([]slog.Attr, 0, 6)
	meta := taskMeta{
		NumIntervalTasks: lt.numIntervalTasks,
	}
	var interTaskInterval time.Duration
	if meta.NumIntervalTasks > 0 {
		interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
	}

	taskBuf := make([]Doer, 0, lt.maxIntervalTasks)

	var enqueueTasks func()
	var updateEnqueueTasksStrategy func()
	{
		floodStrategy := func() {
			for _, task := range taskBuf {
				lt.taskChan <- taskWithMeta{task, intervalID, meta}
			}
		}

		staggerStrategy := func() {
			lt.taskChan <- taskWithMeta{taskBuf[0], intervalID, meta}

			for _, task := range taskBuf[1:] {
				time.Sleep(interTaskInterval)
				lt.taskChan <- taskWithMeta{task, time.Now(), meta}
			}
		}

		updateEnqueueTasksStrategy = func() {
			if interTaskInterval <= skipInterTaskSchedulingThreshold {
				enqueueTasks = floodStrategy
			} else {
				enqueueTasks = staggerStrategy
			}
		}
	}
	updateEnqueueTasksStrategy()

	var delay time.Duration

	// stopping routine runs on return
	// flushing as much as possible
	defer func() {

		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for running tasks to stop",
		)
		lt.resultWaitGroup.Wait()

		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping result handler routine",
		)

		// signal for result handler routines to stop
		close(lt.resultsChan)

		// signal for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping workers",
		)
		for i := 0; i < numSpawnedWorkers; i++ {
			close(lt.pauseChans[i])
		}

		// wait for result handler routines to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for result handler routines to stop",
		)
		wg.Wait()

		// wait for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for workers to stop",
		)
		lt.workerWaitGroup.Wait()

		lt.logger.LogAttrs(ctx, slog.LevelInfo,
			"loadtest stopped",
		)
	}()

	// getTaskSlotCount is the task emission back pressure
	// throttle that conveys the number of tasks that
	// are allowed to be un-finished for the performance
	// interval under normal circumstances
	getTaskSlotCount := func() int {
		return maxPendingTasks(numWorkers, numNewTasks)
	}

	// apply initial task buffer limits to the interval semaphore
	taskSlotCount := getTaskSlotCount()
	lt.intervalTasksSema.Release(int64(taskSlotCount))

	configCausesPause := func() bool {
		return meta.NumIntervalTasks <= 0 || numWorkers <= 0
	}

	var paused bool
	var pauseStart time.Time

	handleConfigUpdateAndPauseState := func(cu ConfigUpdate) error {
		for {
			var prepSemaErr error
			var recomputeInterTaskInterval, recomputeTaskSlots bool

			if cu.numWorkers.set {
				recomputeTaskSlots = true

				n := cu.numWorkers.val

				// prevent over committing on the maxWorkers count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxWorkers {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too high"),
						slog.String("remediation_hint", "increase the loadtest MaxWorkers setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxWorkers),
					)
					n = lt.maxWorkers
				}

				if n > numWorkers {

					// unpause workers
					for i := numWorkers; i < numSpawnedWorkers; i++ {
						lt.pauseChans[i] <- struct{}{}
					}

					// spawn new workers if needed
					for i := numSpawnedWorkers; i < n; i++ {
						lt.addWorker(ctx, i)
						numSpawnedWorkers++
					}
				} else if n < numWorkers {

					// pause workers if needed
					for i := numWorkers - 1; i >= n; i-- {
						lt.pauseChans[i] <- struct{}{}
					}
				}

				configChanges = append(configChanges,
					slog.Int("old_num_workers", numWorkers),
					slog.Int("new_num_workers", n),
				)
				numWorkers = n
			}

			if cu.numIntervalTasks.set {
				recomputeInterTaskInterval = true
				recomputeTaskSlots = true

				n := cu.numIntervalTasks.val

				// prevent over committing on the maxIntervalTasks count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxIntervalTasks {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too high"),
						slog.String("remediation_hint", "increase the loadtest MaxIntervalTasks setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxIntervalTasks),
					)
					n = lt.maxIntervalTasks
				}

				configChanges = append(configChanges,
					slog.Int("old_num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("new_num_interval_tasks", n),
				)
				numNewTasks = n
				meta.NumIntervalTasks = n
			}

			if cu.interval.set {
				recomputeInterTaskInterval = true

				n := cu.interval.val

				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: interval",
						slog.String("reason", "update tried to set interval too low"),
						slog.String("remediation_taken", "using min value"),
						slog.String("requested", n.String()),
						slog.String("min", time.Duration(0).String()),
					)
					n = 0
				}

				configChanges = append(configChanges,
					slog.String("old_interval", interval.String()),
					slog.String("new_interval", n.String()),
				)
				interval = n
			}

			// && clause: protects against divide by zero
			if recomputeInterTaskInterval && meta.NumIntervalTasks > 0 {
				interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
				updateEnqueueTasksStrategy()
			}

			if recomputeTaskSlots {
				if newTaskSlotCount := getTaskSlotCount(); newTaskSlotCount != taskSlotCount {

					if newTaskSlotCount > taskSlotCount {
						lt.intervalTasksSema.Release(int64(newTaskSlotCount - taskSlotCount))
					} else {
						prepSemaErr = lt.intervalTasksSema.Acquire(ctx, int64(taskSlotCount-newTaskSlotCount))
						if prepSemaErr != nil {
							lt.logger.LogAttrs(ctx, slog.LevelError,
								"loadtest config update: failed to pre-acquire load generation slots",
								slog.Any("error", prepSemaErr),
							)

							// not returning and error... yet
							// going to let config update log statement occur and then report the error present in prepSemaErr
						}
					}

					taskSlotCount = newTaskSlotCount
				}
			}

			if !cu.onStartup {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"loadtest config updated",
					configChanges...,
				)
			}
			configChanges = configChanges[:0]

			if prepSemaErr != nil {
				return prepSemaErr
			}

			// pause load generation if unable to schedule anything
			if configCausesPause() {

				if !paused {
					paused = true
					pauseStart = time.Now().UTC()

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"pausing load generation",
						slog.Int("num_interval_tasks", meta.NumIntervalTasks),
						slog.Int("num_workers", numWorkers),
						slog.String("paused_at", pauseStart.String()),
					)
				}

				// duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
				// that follows
				//
				// ref: https://go.dev/ref/spec#Select_statements
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				default:
				}
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				case cu = <-cfgUpdateChan:
					continue
				}
			}

			if paused {
				paused = false
				intervalID = time.Now()

				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"resuming load generation",
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
					slog.String("paused_at", pauseStart.String()),
					slog.String("resumed_at", intervalID.UTC().String()),
				)
			}

			return nil
		}
	}

	if configCausesPause() {
		if err := handleConfigUpdateAndPauseState(ConfigUpdate{onStartup: true}); err != nil {
			if err == errLoadtestContextDone {
				return nil
			}
			return err
		}
	}

	// start workers just before starting task scheduling
	for i := 0; i < numWorkers; i++ {
		lt.addWorker(ctx, i)
		numSpawnedWorkers++
	}

	// main task scheduling loop
	for {

		// Not duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
		// that follows because this is a ordered sequence where all selects in the sequence have non-blocking default cases.
		//
		// The odds of thread switching between these selects is minimal as is the impact of having to wait at most one more cycle
		// to short circuit and return.
		//
		// ref: https://go.dev/ref/spec#Select_statements
		select {
		case <-ctxDone:
			return nil
		default:
		}
		select {
		case cu := <-cfgUpdateChan:
			if err := handleConfigUpdateAndPauseState(cu); err != nil {
				if err == errLoadtestContextDone {
					return nil
				}
				return err
			}

			// re-loop
			continue
		default:
			// continue with load generation
		}

		taskBufSize := 0
		taskBuf = taskBuf[:taskBufSize]

		if taskBufSize < numNewTasks {
			maxSize := numNewTasks - taskBufSize
			n := taskReader.ReadTasks(taskBuf[taskBufSize:numNewTasks:numNewTasks])
			if n < 0 || n > maxSize {
				panic(ErrBadReadTasksImpl)
			}
			if n == 0 {

				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"stopping loadtest: ReadTasks did not load enough tasks",
					slog.Int("final_task_delta", 0),
				)

				return nil
			}

			taskBufSize += n
			taskBuf = taskBuf[:taskBufSize]
		}

		// acquire load generation opportunity slots ( smooths bursts )
		// if not done already
		if lt.intervalTasksSema.Acquire(ctx, int64(taskBufSize)) != nil {
			return nil
		}

		lt.resultWaitGroup.Add(taskBufSize + 1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
		lt.resultsChan <- taskResult{
			Meta: taskMeta{
				// IntervalID: intervalID, // not required unless in a debug context
				SampleSize: taskBufSize,
			},
		}

		meta.IntervalID = intervalID

		enqueueTasks()

		if numNewTasks > taskBufSize {
			// must have hit the end of ReadTasks iterator
			// increase numTasks total by actual number queued
			// and stop traffic generation
			// numTasks += taskBufSize // this line only has an effect except in a retries enabled context
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"stopping loadtest: ReadTasks did not load enough tasks",
				slog.Int("final_task_delta", taskBufSize),
			)
			return nil
		}

		taskBuf = taskBuf[:0]

		numTasks += taskBufSize

		meta.Lag = 0

		// wait for next interval time to exist
		nextIntervalID := intervalID.Add(interval)
		realNow := time.Now()
		delay = nextIntervalID.Sub(realNow)
		if delay > 0 {
			time.Sleep(delay)
			intervalID = nextIntervalID
			continue
		}

		if delay < 0 {
			intervalID = realNow

			lag := -delay
			meta.Lag = lag

			lt.resultWaitGroup.Add(1)
			lt.resultsChan <- taskResult{
				Meta: taskMeta{
					// IntervalID: intervalID, // not required unless in a debug context
					Lag: lag,
				},
			}

		}
	}
}

func (lt *Loadtest) run_retriesDisabled_maxTasksNotGTZero_metricsDisabled(ctx context.Context, _ *error) error {

	cfgUpdateChan := lt.cfgUpdateChan
	defer close(cfgUpdateChan)

	lt.startTime = time.Now()

	lt.logger.LogAttrs(ctx, slog.LevelInfo,
		"starting loadtest",
		slog.Any("config", lt.loadtestConfigAsJson()),
	)

	numWorkers := lt.numWorkers
	numSpawnedWorkers := 0

	// numTasks is the total number of tasks
	// scheduled to run ( including retries )
	var numTasks int

	intervalID := time.Now()

	interval := lt.interval
	numNewTasks := lt.numIntervalTasks
	ctxDone := ctx.Done()
	taskReader := lt.taskReader
	configChanges := make([]slog.Attr, 0, 6)
	meta := taskMeta{
		NumIntervalTasks: lt.numIntervalTasks,
	}
	var interTaskInterval time.Duration
	if meta.NumIntervalTasks > 0 {
		interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
	}

	taskBuf := make([]Doer, 0, lt.maxIntervalTasks)

	var enqueueTasks func()
	var updateEnqueueTasksStrategy func()
	{
		floodStrategy := func() {
			for _, task := range taskBuf {
				lt.taskChan <- taskWithMeta{task, intervalID, meta}
			}
		}

		staggerStrategy := func() {
			lt.taskChan <- taskWithMeta{taskBuf[0], intervalID, meta}

			for _, task := range taskBuf[1:] {
				time.Sleep(interTaskInterval)
				lt.taskChan <- taskWithMeta{task, time.Now(), meta}
			}
		}

		updateEnqueueTasksStrategy = func() {
			if interTaskInterval <= skipInterTaskSchedulingThreshold {
				enqueueTasks = floodStrategy
			} else {
				enqueueTasks = staggerStrategy
			}
		}
	}
	updateEnqueueTasksStrategy()

	var delay time.Duration

	// stopping routine runs on return
	// flushing as much as possible
	defer func() {

		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for running tasks to stop",
		)
		lt.resultWaitGroup.Wait()

		// signal for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"stopping workers",
		)
		for i := 0; i < numSpawnedWorkers; i++ {
			close(lt.pauseChans[i])
		}

		// wait for workers to stop
		lt.logger.LogAttrs(ctx, slog.LevelDebug,
			"waiting for workers to stop",
		)
		lt.workerWaitGroup.Wait()

		lt.logger.LogAttrs(ctx, slog.LevelInfo,
			"loadtest stopped",
		)
	}()

	// getTaskSlotCount is the task emission back pressure
	// throttle that conveys the number of tasks that
	// are allowed to be un-finished for the performance
	// interval under normal circumstances
	getTaskSlotCount := func() int {
		return maxPendingTasks(numWorkers, numNewTasks)
	}

	// apply initial task buffer limits to the interval semaphore
	taskSlotCount := getTaskSlotCount()
	lt.intervalTasksSema.Release(int64(taskSlotCount))

	configCausesPause := func() bool {
		return meta.NumIntervalTasks <= 0 || numWorkers <= 0
	}

	var paused bool
	var pauseStart time.Time

	handleConfigUpdateAndPauseState := func(cu ConfigUpdate) error {
		for {
			var prepSemaErr error
			var recomputeInterTaskInterval, recomputeTaskSlots bool

			if cu.numWorkers.set {
				recomputeTaskSlots = true

				n := cu.numWorkers.val

				// prevent over committing on the maxWorkers count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxWorkers {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numWorkers",
						slog.String("reason", "update tried to set numWorkers too high"),
						slog.String("remediation_hint", "increase the loadtest MaxWorkers setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxWorkers),
					)
					n = lt.maxWorkers
				}

				if n > numWorkers {

					// unpause workers
					for i := numWorkers; i < numSpawnedWorkers; i++ {
						lt.pauseChans[i] <- struct{}{}
					}

					// spawn new workers if needed
					for i := numSpawnedWorkers; i < n; i++ {
						lt.addWorker(ctx, i)
						numSpawnedWorkers++
					}
				} else if n < numWorkers {

					// pause workers if needed
					for i := numWorkers - 1; i >= n; i-- {
						lt.pauseChans[i] <- struct{}{}
					}
				}

				configChanges = append(configChanges,
					slog.Int("old_num_workers", numWorkers),
					slog.Int("new_num_workers", n),
				)
				numWorkers = n
			}

			if cu.numIntervalTasks.set {
				recomputeInterTaskInterval = true
				recomputeTaskSlots = true

				n := cu.numIntervalTasks.val

				// prevent over committing on the maxIntervalTasks count
				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too low"),
						slog.String("remediation_taken", "using min value"),
						slog.Int("requested", n),
						slog.Int("min", 0),
					)
					n = 0
				} else if n > lt.maxIntervalTasks {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: numIntervalTasks",
						slog.String("reason", "update tried to set numIntervalTasks too high"),
						slog.String("remediation_hint", "increase the loadtest MaxIntervalTasks setting"),
						slog.String("remediation_taken", "using max value"),
						slog.Int("requested", n),
						slog.Int("max", lt.maxIntervalTasks),
					)
					n = lt.maxIntervalTasks
				}

				configChanges = append(configChanges,
					slog.Int("old_num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("new_num_interval_tasks", n),
				)
				numNewTasks = n
				meta.NumIntervalTasks = n
			}

			if cu.interval.set {
				recomputeInterTaskInterval = true

				n := cu.interval.val

				if n < 0 {
					lt.logger.LogAttrs(ctx, slog.LevelError,
						"config update not within loadtest boundary conditions: interval",
						slog.String("reason", "update tried to set interval too low"),
						slog.String("remediation_taken", "using min value"),
						slog.String("requested", n.String()),
						slog.String("min", time.Duration(0).String()),
					)
					n = 0
				}

				configChanges = append(configChanges,
					slog.String("old_interval", interval.String()),
					slog.String("new_interval", n.String()),
				)
				interval = n
			}

			// && clause: protects against divide by zero
			if recomputeInterTaskInterval && meta.NumIntervalTasks > 0 {
				interTaskInterval = interval / time.Duration(meta.NumIntervalTasks)
				updateEnqueueTasksStrategy()
			}

			if recomputeTaskSlots {
				if newTaskSlotCount := getTaskSlotCount(); newTaskSlotCount != taskSlotCount {

					if newTaskSlotCount > taskSlotCount {
						lt.intervalTasksSema.Release(int64(newTaskSlotCount - taskSlotCount))
					} else {
						prepSemaErr = lt.intervalTasksSema.Acquire(ctx, int64(taskSlotCount-newTaskSlotCount))
						if prepSemaErr != nil {
							lt.logger.LogAttrs(ctx, slog.LevelError,
								"loadtest config update: failed to pre-acquire load generation slots",
								slog.Any("error", prepSemaErr),
							)

							// not returning and error... yet
							// going to let config update log statement occur and then report the error present in prepSemaErr
						}
					}

					taskSlotCount = newTaskSlotCount
				}
			}

			if !cu.onStartup {
				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"loadtest config updated",
					configChanges...,
				)
			}
			configChanges = configChanges[:0]

			if prepSemaErr != nil {
				return prepSemaErr
			}

			// pause load generation if unable to schedule anything
			if configCausesPause() {

				if !paused {
					paused = true
					pauseStart = time.Now().UTC()

					lt.logger.LogAttrs(ctx, slog.LevelWarn,
						"pausing load generation",
						slog.Int("num_interval_tasks", meta.NumIntervalTasks),
						slog.Int("num_workers", numWorkers),
						slog.String("paused_at", pauseStart.String()),
					)
				}

				// duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
				// that follows
				//
				// ref: https://go.dev/ref/spec#Select_statements
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				default:
				}
				select {
				case <-ctxDone:
					return errLoadtestContextDone
				case cu = <-cfgUpdateChan:
					continue
				}
			}

			if paused {
				paused = false
				intervalID = time.Now()

				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"resuming load generation",
					slog.Int("num_interval_tasks", meta.NumIntervalTasks),
					slog.Int("num_workers", numWorkers),
					slog.String("paused_at", pauseStart.String()),
					slog.String("resumed_at", intervalID.UTC().String()),
				)
			}

			return nil
		}
	}

	if configCausesPause() {
		if err := handleConfigUpdateAndPauseState(ConfigUpdate{onStartup: true}); err != nil {
			if err == errLoadtestContextDone {
				return nil
			}
			return err
		}
	}

	// start workers just before starting task scheduling
	for i := 0; i < numWorkers; i++ {
		lt.addWorker(ctx, i)
		numSpawnedWorkers++
	}

	// main task scheduling loop
	for {

		// Not duplicating short-circuit signal control processing to give it priority over the randomizing nature of the multi-select
		// that follows because this is a ordered sequence where all selects in the sequence have non-blocking default cases.
		//
		// The odds of thread switching between these selects is minimal as is the impact of having to wait at most one more cycle
		// to short circuit and return.
		//
		// ref: https://go.dev/ref/spec#Select_statements
		select {
		case <-ctxDone:
			return nil
		default:
		}
		select {
		case cu := <-cfgUpdateChan:
			if err := handleConfigUpdateAndPauseState(cu); err != nil {
				if err == errLoadtestContextDone {
					return nil
				}
				return err
			}

			// re-loop
			continue
		default:
			// continue with load generation
		}

		taskBufSize := 0
		taskBuf = taskBuf[:taskBufSize]

		if taskBufSize < numNewTasks {
			maxSize := numNewTasks - taskBufSize
			n := taskReader.ReadTasks(taskBuf[taskBufSize:numNewTasks:numNewTasks])
			if n < 0 || n > maxSize {
				panic(ErrBadReadTasksImpl)
			}
			if n == 0 {

				lt.logger.LogAttrs(ctx, slog.LevelWarn,
					"stopping loadtest: ReadTasks did not load enough tasks",
					slog.Int("final_task_delta", 0),
				)

				return nil
			}

			taskBufSize += n
			taskBuf = taskBuf[:taskBufSize]
		}

		// acquire load generation opportunity slots ( smooths bursts )
		// if not done already
		if lt.intervalTasksSema.Acquire(ctx, int64(taskBufSize)) != nil {
			return nil
		}

		lt.resultWaitGroup.Add(taskBufSize + 1) // +1 because we're sending the expected Sample Size immediately to the results handler before queueing tasks
		lt.resultsChan <- taskResult{
			Meta: taskMeta{
				// IntervalID: intervalID, // not required unless in a debug context
				SampleSize: taskBufSize,
			},
		}

		meta.IntervalID = intervalID

		enqueueTasks()

		if numNewTasks > taskBufSize {
			// must have hit the end of ReadTasks iterator
			// increase numTasks total by actual number queued
			// and stop traffic generation
			// numTasks += taskBufSize // this line only has an effect except in a retries enabled context
			lt.logger.LogAttrs(ctx, slog.LevelWarn,
				"stopping loadtest: ReadTasks did not load enough tasks",
				slog.Int("final_task_delta", taskBufSize),
			)
			return nil
		}

		taskBuf = taskBuf[:0]

		numTasks += taskBufSize

		// wait for next interval time to exist
		nextIntervalID := intervalID.Add(interval)
		realNow := time.Now()
		delay = nextIntervalID.Sub(realNow)
		if delay > 0 {
			time.Sleep(delay)
			intervalID = nextIntervalID
			continue
		}

		if delay < 0 {
			intervalID = realNow

		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryEnabled_maxTasksGTZero_percentileEnabled_varianceEnabled() func(metricRecord_retryEnabled_maxTasksGTZero_percentileEnabled_varianceEnabled) {

	var queuePercentiles, taskPercentiles [numPercentiles]string

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryEnabled_maxTasksGTZero_percentileEnabled_varianceEnabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		var percent string
		{
			high := mr.totalNumTasks * percentDonePrecisionFactor / lt.maxTasks
			low := high % (percentDonePrecisionFactor / 100)
			high /= (percentDonePrecisionFactor / 100)

			var sep string
			if low < 10 {
				sep = ".0"
			} else {
				sep = "."
			}

			percent = strconv.Itoa(high) + sep + strconv.Itoa(low)
		}

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		mr.latencies.queue.readPercentileStrings(&queuePercentiles)
		mr.latencies.task.readPercentileStrings(&taskPercentiles)

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numRetry),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			queuePercentiles[0],
			queuePercentiles[1],
			queuePercentiles[2],
			queuePercentiles[3],
			queuePercentiles[4],
			queuePercentiles[5],
			queuePercentiles[6],
			queuePercentiles[7],
			queuePercentiles[8],
			queuePercentiles[9],
			taskPercentiles[0],
			taskPercentiles[1],
			taskPercentiles[2],
			taskPercentiles[3],
			taskPercentiles[4],
			taskPercentiles[5],
			taskPercentiles[6],
			taskPercentiles[7],
			taskPercentiles[8],
			taskPercentiles[9],
			varianceFloatString(mr.welfords.queue.Variance(mr.numTasks)),
			varianceFloatString(mr.welfords.task.Variance(mr.numTasks)),
			percent,
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryEnabled_maxTasksGTZero_percentileEnabled_varianceDisabled() func(metricRecord_retryEnabled_maxTasksGTZero_percentileEnabled_varianceDisabled) {

	var queuePercentiles, taskPercentiles [numPercentiles]string

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryEnabled_maxTasksGTZero_percentileEnabled_varianceDisabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		var percent string
		{
			high := mr.totalNumTasks * percentDonePrecisionFactor / lt.maxTasks
			low := high % (percentDonePrecisionFactor / 100)
			high /= (percentDonePrecisionFactor / 100)

			var sep string
			if low < 10 {
				sep = ".0"
			} else {
				sep = "."
			}

			percent = strconv.Itoa(high) + sep + strconv.Itoa(low)
		}

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		mr.latencies.queue.readPercentileStrings(&queuePercentiles)
		mr.latencies.task.readPercentileStrings(&taskPercentiles)

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numRetry),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			queuePercentiles[0],
			queuePercentiles[1],
			queuePercentiles[2],
			queuePercentiles[3],
			queuePercentiles[4],
			queuePercentiles[5],
			queuePercentiles[6],
			queuePercentiles[7],
			queuePercentiles[8],
			queuePercentiles[9],
			taskPercentiles[0],
			taskPercentiles[1],
			taskPercentiles[2],
			taskPercentiles[3],
			taskPercentiles[4],
			taskPercentiles[5],
			taskPercentiles[6],
			taskPercentiles[7],
			taskPercentiles[8],
			taskPercentiles[9],
			percent,
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryEnabled_maxTasksGTZero_percentileDisabled_varianceEnabled() func(metricRecord_retryEnabled_maxTasksGTZero_percentileDisabled_varianceEnabled) {

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryEnabled_maxTasksGTZero_percentileDisabled_varianceEnabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		var percent string
		{
			high := mr.totalNumTasks * percentDonePrecisionFactor / lt.maxTasks
			low := high % (percentDonePrecisionFactor / 100)
			high /= (percentDonePrecisionFactor / 100)

			var sep string
			if low < 10 {
				sep = ".0"
			} else {
				sep = "."
			}

			percent = strconv.Itoa(high) + sep + strconv.Itoa(low)
		}

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numRetry),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			varianceFloatString(mr.welfords.queue.Variance(mr.numTasks)),
			varianceFloatString(mr.welfords.task.Variance(mr.numTasks)),
			percent,
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryEnabled_maxTasksGTZero_percentileDisabled_varianceDisabled() func(metricRecord_retryEnabled_maxTasksGTZero_percentileDisabled_varianceDisabled) {

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryEnabled_maxTasksGTZero_percentileDisabled_varianceDisabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		var percent string
		{
			high := mr.totalNumTasks * percentDonePrecisionFactor / lt.maxTasks
			low := high % (percentDonePrecisionFactor / 100)
			high /= (percentDonePrecisionFactor / 100)

			var sep string
			if low < 10 {
				sep = ".0"
			} else {
				sep = "."
			}

			percent = strconv.Itoa(high) + sep + strconv.Itoa(low)
		}

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numRetry),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			percent,
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled() func(metricRecord_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled) {

	var queuePercentiles, taskPercentiles [numPercentiles]string

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		mr.latencies.queue.readPercentileStrings(&queuePercentiles)
		mr.latencies.task.readPercentileStrings(&taskPercentiles)

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numRetry),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			queuePercentiles[0],
			queuePercentiles[1],
			queuePercentiles[2],
			queuePercentiles[3],
			queuePercentiles[4],
			queuePercentiles[5],
			queuePercentiles[6],
			queuePercentiles[7],
			queuePercentiles[8],
			queuePercentiles[9],
			taskPercentiles[0],
			taskPercentiles[1],
			taskPercentiles[2],
			taskPercentiles[3],
			taskPercentiles[4],
			taskPercentiles[5],
			taskPercentiles[6],
			taskPercentiles[7],
			taskPercentiles[8],
			taskPercentiles[9],
			varianceFloatString(mr.welfords.queue.Variance(mr.numTasks)),
			varianceFloatString(mr.welfords.task.Variance(mr.numTasks)),
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled() func(metricRecord_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled) {

	var queuePercentiles, taskPercentiles [numPercentiles]string

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		mr.latencies.queue.readPercentileStrings(&queuePercentiles)
		mr.latencies.task.readPercentileStrings(&taskPercentiles)

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numRetry),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			queuePercentiles[0],
			queuePercentiles[1],
			queuePercentiles[2],
			queuePercentiles[3],
			queuePercentiles[4],
			queuePercentiles[5],
			queuePercentiles[6],
			queuePercentiles[7],
			queuePercentiles[8],
			queuePercentiles[9],
			taskPercentiles[0],
			taskPercentiles[1],
			taskPercentiles[2],
			taskPercentiles[3],
			taskPercentiles[4],
			taskPercentiles[5],
			taskPercentiles[6],
			taskPercentiles[7],
			taskPercentiles[8],
			taskPercentiles[9],
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled() func(metricRecord_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled) {

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numRetry),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			varianceFloatString(mr.welfords.queue.Variance(mr.numTasks)),
			varianceFloatString(mr.welfords.task.Variance(mr.numTasks)),
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled() func(metricRecord_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled) {

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numRetry),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryDisabled_maxTasksGTZero_percentileEnabled_varianceEnabled() func(metricRecord_retryDisabled_maxTasksGTZero_percentileEnabled_varianceEnabled) {

	var queuePercentiles, taskPercentiles [numPercentiles]string

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryDisabled_maxTasksGTZero_percentileEnabled_varianceEnabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		var percent string
		{
			high := mr.totalNumTasks * percentDonePrecisionFactor / lt.maxTasks
			low := high % (percentDonePrecisionFactor / 100)
			high /= (percentDonePrecisionFactor / 100)

			var sep string
			if low < 10 {
				sep = ".0"
			} else {
				sep = "."
			}

			percent = strconv.Itoa(high) + sep + strconv.Itoa(low)
		}

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		mr.latencies.queue.readPercentileStrings(&queuePercentiles)
		mr.latencies.task.readPercentileStrings(&taskPercentiles)

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			queuePercentiles[0],
			queuePercentiles[1],
			queuePercentiles[2],
			queuePercentiles[3],
			queuePercentiles[4],
			queuePercentiles[5],
			queuePercentiles[6],
			queuePercentiles[7],
			queuePercentiles[8],
			queuePercentiles[9],
			taskPercentiles[0],
			taskPercentiles[1],
			taskPercentiles[2],
			taskPercentiles[3],
			taskPercentiles[4],
			taskPercentiles[5],
			taskPercentiles[6],
			taskPercentiles[7],
			taskPercentiles[8],
			taskPercentiles[9],
			varianceFloatString(mr.welfords.queue.Variance(mr.numTasks)),
			varianceFloatString(mr.welfords.task.Variance(mr.numTasks)),
			percent,
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryDisabled_maxTasksGTZero_percentileEnabled_varianceDisabled() func(metricRecord_retryDisabled_maxTasksGTZero_percentileEnabled_varianceDisabled) {

	var queuePercentiles, taskPercentiles [numPercentiles]string

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryDisabled_maxTasksGTZero_percentileEnabled_varianceDisabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		var percent string
		{
			high := mr.totalNumTasks * percentDonePrecisionFactor / lt.maxTasks
			low := high % (percentDonePrecisionFactor / 100)
			high /= (percentDonePrecisionFactor / 100)

			var sep string
			if low < 10 {
				sep = ".0"
			} else {
				sep = "."
			}

			percent = strconv.Itoa(high) + sep + strconv.Itoa(low)
		}

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		mr.latencies.queue.readPercentileStrings(&queuePercentiles)
		mr.latencies.task.readPercentileStrings(&taskPercentiles)

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			queuePercentiles[0],
			queuePercentiles[1],
			queuePercentiles[2],
			queuePercentiles[3],
			queuePercentiles[4],
			queuePercentiles[5],
			queuePercentiles[6],
			queuePercentiles[7],
			queuePercentiles[8],
			queuePercentiles[9],
			taskPercentiles[0],
			taskPercentiles[1],
			taskPercentiles[2],
			taskPercentiles[3],
			taskPercentiles[4],
			taskPercentiles[5],
			taskPercentiles[6],
			taskPercentiles[7],
			taskPercentiles[8],
			taskPercentiles[9],
			percent,
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryDisabled_maxTasksGTZero_percentileDisabled_varianceEnabled() func(metricRecord_retryDisabled_maxTasksGTZero_percentileDisabled_varianceEnabled) {

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryDisabled_maxTasksGTZero_percentileDisabled_varianceEnabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		var percent string
		{
			high := mr.totalNumTasks * percentDonePrecisionFactor / lt.maxTasks
			low := high % (percentDonePrecisionFactor / 100)
			high /= (percentDonePrecisionFactor / 100)

			var sep string
			if low < 10 {
				sep = ".0"
			} else {
				sep = "."
			}

			percent = strconv.Itoa(high) + sep + strconv.Itoa(low)
		}

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			varianceFloatString(mr.welfords.queue.Variance(mr.numTasks)),
			varianceFloatString(mr.welfords.task.Variance(mr.numTasks)),
			percent,
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryDisabled_maxTasksGTZero_percentileDisabled_varianceDisabled() func(metricRecord_retryDisabled_maxTasksGTZero_percentileDisabled_varianceDisabled) {

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryDisabled_maxTasksGTZero_percentileDisabled_varianceDisabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		var percent string
		{
			high := mr.totalNumTasks * percentDonePrecisionFactor / lt.maxTasks
			low := high % (percentDonePrecisionFactor / 100)
			high /= (percentDonePrecisionFactor / 100)

			var sep string
			if low < 10 {
				sep = ".0"
			} else {
				sep = "."
			}

			percent = strconv.Itoa(high) + sep + strconv.Itoa(low)
		}

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			percent,
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled() func(metricRecord_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled) {

	var queuePercentiles, taskPercentiles [numPercentiles]string

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		mr.latencies.queue.readPercentileStrings(&queuePercentiles)
		mr.latencies.task.readPercentileStrings(&taskPercentiles)

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			queuePercentiles[0],
			queuePercentiles[1],
			queuePercentiles[2],
			queuePercentiles[3],
			queuePercentiles[4],
			queuePercentiles[5],
			queuePercentiles[6],
			queuePercentiles[7],
			queuePercentiles[8],
			queuePercentiles[9],
			taskPercentiles[0],
			taskPercentiles[1],
			taskPercentiles[2],
			taskPercentiles[3],
			taskPercentiles[4],
			taskPercentiles[5],
			taskPercentiles[6],
			taskPercentiles[7],
			taskPercentiles[8],
			taskPercentiles[9],
			varianceFloatString(mr.welfords.queue.Variance(mr.numTasks)),
			varianceFloatString(mr.welfords.task.Variance(mr.numTasks)),
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled() func(metricRecord_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled) {

	var queuePercentiles, taskPercentiles [numPercentiles]string

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		mr.latencies.queue.readPercentileStrings(&queuePercentiles)
		mr.latencies.task.readPercentileStrings(&taskPercentiles)

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			queuePercentiles[0],
			queuePercentiles[1],
			queuePercentiles[2],
			queuePercentiles[3],
			queuePercentiles[4],
			queuePercentiles[5],
			queuePercentiles[6],
			queuePercentiles[7],
			queuePercentiles[8],
			queuePercentiles[9],
			taskPercentiles[0],
			taskPercentiles[1],
			taskPercentiles[2],
			taskPercentiles[3],
			taskPercentiles[4],
			taskPercentiles[5],
			taskPercentiles[6],
			taskPercentiles[7],
			taskPercentiles[8],
			taskPercentiles[9],
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled() func(metricRecord_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled) {

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
			varianceFloatString(mr.welfords.queue.Variance(mr.numTasks)),
			varianceFloatString(mr.welfords.task.Variance(mr.numTasks)),
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) writeOutputCsvRow_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled() func(metricRecord_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled) {

	var bigAvgQueueLatency, bigAvgTaskLatency big.Int

	return func(mr metricRecord_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled) {

		cd := &lt.csvData
		if cd.writeErr != nil {
			return
		}

		now := time.Now()

		bigNumTasks := big.NewInt(int64(mr.numTasks))

		fields := []string{
			timeToUnixNanoString(now),
			timeToUnixNanoString(mr.intervalID),
			strconv.Itoa(mr.numIntervalTasks),
			durationToNanoString(mr.lag),
			durationToNanoString(mr.sumLag),
			strconv.Itoa(mr.numTasks),
			strconv.Itoa(mr.numPass),
			strconv.Itoa(mr.numFail),
			strconv.Itoa(mr.numPanic),
			durationToNanoString(mr.minQueueDuration),
			durationToNanoString(time.Duration(bigAvgQueueLatency.Div(&mr.sumQueueDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxQueueDuration),
			durationToNanoString(mr.minTaskDuration),
			durationToNanoString(time.Duration(bigAvgTaskLatency.Div(&mr.sumTaskDuration, bigNumTasks).Int64())),
			durationToNanoString(mr.maxTaskDuration),
		}

		if err := cd.writer.Write(fields); err != nil {
			cd.setErr(err) // sets error state in multiple goroutine safe way
		}
	}
}

func (lt *Loadtest) resultsHandler_retryEnabled_maxTasksGTZero_percentileEnabled_varianceEnabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryEnabled_maxTasksGTZero_percentileEnabled_varianceEnabled
		mr.latencies = lt.latencies
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryEnabled_maxTasksGTZero_percentileEnabled_varianceEnabled()
			writeRow = func() {

				mr.totalNumTasks += mr.numTasks

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)
			mr.numRetry += int(tr.RetryQueued)

			mr.numTasks++

			mr.welfords.queue.Update(mr.numTasks, float64(tr.QueueDuration))
			mr.welfords.task.Update(mr.numTasks, float64(tr.TaskDuration))

			mr.latencies.queue.add(tr.QueueDuration)
			mr.latencies.task.add(tr.TaskDuration)

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryEnabled_maxTasksGTZero_percentileEnabled_varianceDisabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryEnabled_maxTasksGTZero_percentileEnabled_varianceDisabled
		mr.latencies = lt.latencies
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryEnabled_maxTasksGTZero_percentileEnabled_varianceDisabled()
			writeRow = func() {

				mr.totalNumTasks += mr.numTasks

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)
			mr.numRetry += int(tr.RetryQueued)

			mr.numTasks++

			mr.latencies.queue.add(tr.QueueDuration)
			mr.latencies.task.add(tr.TaskDuration)

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryEnabled_maxTasksGTZero_percentileDisabled_varianceEnabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryEnabled_maxTasksGTZero_percentileDisabled_varianceEnabled
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryEnabled_maxTasksGTZero_percentileDisabled_varianceEnabled()
			writeRow = func() {

				mr.totalNumTasks += mr.numTasks

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)
			mr.numRetry += int(tr.RetryQueued)

			mr.numTasks++

			mr.welfords.queue.Update(mr.numTasks, float64(tr.QueueDuration))
			mr.welfords.task.Update(mr.numTasks, float64(tr.TaskDuration))

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryEnabled_maxTasksGTZero_percentileDisabled_varianceDisabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryEnabled_maxTasksGTZero_percentileDisabled_varianceDisabled
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryEnabled_maxTasksGTZero_percentileDisabled_varianceDisabled()
			writeRow = func() {

				mr.totalNumTasks += mr.numTasks

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)
			mr.numRetry += int(tr.RetryQueued)

			mr.numTasks++

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled
		mr.latencies = lt.latencies
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled()
			writeRow = func() {

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)
			mr.numRetry += int(tr.RetryQueued)

			mr.numTasks++

			mr.welfords.queue.Update(mr.numTasks, float64(tr.QueueDuration))
			mr.welfords.task.Update(mr.numTasks, float64(tr.TaskDuration))

			mr.latencies.queue.add(tr.QueueDuration)
			mr.latencies.task.add(tr.TaskDuration)

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled
		mr.latencies = lt.latencies
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryEnabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled()
			writeRow = func() {

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)
			mr.numRetry += int(tr.RetryQueued)

			mr.numTasks++

			mr.latencies.queue.add(tr.QueueDuration)
			mr.latencies.task.add(tr.TaskDuration)

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled()
			writeRow = func() {

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)
			mr.numRetry += int(tr.RetryQueued)

			mr.numTasks++

			mr.welfords.queue.Update(mr.numTasks, float64(tr.QueueDuration))
			mr.welfords.task.Update(mr.numTasks, float64(tr.TaskDuration))

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryEnabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled()
			writeRow = func() {

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)
			mr.numRetry += int(tr.RetryQueued)

			mr.numTasks++

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryDisabled_maxTasksGTZero_percentileEnabled_varianceEnabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryDisabled_maxTasksGTZero_percentileEnabled_varianceEnabled
		mr.latencies = lt.latencies
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryDisabled_maxTasksGTZero_percentileEnabled_varianceEnabled()
			writeRow = func() {

				mr.totalNumTasks += mr.numTasks

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)

			mr.numTasks++

			mr.welfords.queue.Update(mr.numTasks, float64(tr.QueueDuration))
			mr.welfords.task.Update(mr.numTasks, float64(tr.TaskDuration))

			mr.latencies.queue.add(tr.QueueDuration)
			mr.latencies.task.add(tr.TaskDuration)

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryDisabled_maxTasksGTZero_percentileEnabled_varianceDisabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryDisabled_maxTasksGTZero_percentileEnabled_varianceDisabled
		mr.latencies = lt.latencies
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryDisabled_maxTasksGTZero_percentileEnabled_varianceDisabled()
			writeRow = func() {

				mr.totalNumTasks += mr.numTasks

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)

			mr.numTasks++

			mr.latencies.queue.add(tr.QueueDuration)
			mr.latencies.task.add(tr.TaskDuration)

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryDisabled_maxTasksGTZero_percentileDisabled_varianceEnabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryDisabled_maxTasksGTZero_percentileDisabled_varianceEnabled
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryDisabled_maxTasksGTZero_percentileDisabled_varianceEnabled()
			writeRow = func() {

				mr.totalNumTasks += mr.numTasks

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)

			mr.numTasks++

			mr.welfords.queue.Update(mr.numTasks, float64(tr.QueueDuration))
			mr.welfords.task.Update(mr.numTasks, float64(tr.TaskDuration))

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryDisabled_maxTasksGTZero_percentileDisabled_varianceDisabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryDisabled_maxTasksGTZero_percentileDisabled_varianceDisabled
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryDisabled_maxTasksGTZero_percentileDisabled_varianceDisabled()
			writeRow = func() {

				mr.totalNumTasks += mr.numTasks

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)

			mr.numTasks++

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled
		mr.latencies = lt.latencies
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceEnabled()
			writeRow = func() {

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)

			mr.numTasks++

			mr.welfords.queue.Update(mr.numTasks, float64(tr.QueueDuration))
			mr.welfords.task.Update(mr.numTasks, float64(tr.TaskDuration))

			mr.latencies.queue.add(tr.QueueDuration)
			mr.latencies.task.add(tr.TaskDuration)

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled
		mr.latencies = lt.latencies
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryDisabled_maxTasksNotGTZero_percentileEnabled_varianceDisabled()
			writeRow = func() {

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)

			mr.numTasks++

			mr.latencies.queue.add(tr.QueueDuration)
			mr.latencies.task.add(tr.TaskDuration)

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceEnabled()
			writeRow = func() {

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)

			mr.numTasks++

			mr.welfords.queue.Update(mr.numTasks, float64(tr.QueueDuration))
			mr.welfords.task.Update(mr.numTasks, float64(tr.TaskDuration))

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}

func (lt *Loadtest) resultsHandler_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled() func() {

	// construct ring buffer of sample sizes (ss)
	ssSize := lt.maxLiveSamples
	ss := make([]int, ssSize)
	var ssNextWriteIdx int
	var ssReadIdx int

	return func() {
		cd := &lt.csvData
		var mr metricRecord_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled
		mr.reset()

		var writeRow func()
		{
			f := lt.writeOutputCsvRow_retryDisabled_maxTasksNotGTZero_percentileDisabled_varianceDisabled()
			writeRow = func() {

				f(mr)
			}
		}

		cd.flushDeadline = time.Now().Add(cd.flushInterval)

		for {
			tr, ok := <-lt.resultsChan
			if !ok {
				if cd.writeErr == nil && mr.numTasks > 0 {
					writeRow()
				}
				return
			}

			lt.resultWaitGroup.Done()

			if cd.writeErr != nil {
				continue
			}

			if tr.taskResultFlags.isZero() {

				mr.sumLag += tr.Meta.Lag

				if tr.Meta.SampleSize > 0 {
					ss[ssNextWriteIdx] = tr.Meta.SampleSize

					// advance write pointer forward
					ssNextWriteIdx = (ssNextWriteIdx + 1) % ssSize
				}

				continue
			}

			if mr.intervalID.Before(tr.Meta.IntervalID) {
				mr.intervalID = tr.Meta.IntervalID
				mr.numIntervalTasks = tr.Meta.NumIntervalTasks
				mr.lag = tr.Meta.Lag
			}

			if mr.minQueueDuration > tr.QueueDuration {
				mr.minQueueDuration = tr.QueueDuration
			}

			if mr.minTaskDuration > tr.TaskDuration {
				mr.minTaskDuration = tr.TaskDuration
			}

			if mr.maxTaskDuration < tr.TaskDuration {
				mr.maxTaskDuration = tr.TaskDuration
			}

			if mr.maxQueueDuration < tr.QueueDuration {
				mr.maxQueueDuration = tr.QueueDuration
			}

			mr.sumQueueDuration.Add(&mr.sumQueueDuration, big.NewInt(int64(tr.QueueDuration)))
			mr.sumTaskDuration.Add(&mr.sumTaskDuration, big.NewInt(int64(tr.TaskDuration)))
			mr.numPass += int(tr.Passed)
			mr.numFail += int(tr.Errored)
			mr.numPanic += int(tr.Panicked)

			mr.numTasks++

			if mr.numTasks >= ss[ssReadIdx] {

				writeRow()
				mr.reset()

				// advance read pointer forward
				ssReadIdx = (ssReadIdx + 1) % ssSize

				if cd.writeErr == nil && !cd.flushDeadline.After(time.Now()) {
					cd.writer.Flush()
					if err := cd.writer.Error(); err != nil {
						cd.setErr(err) // sets error state in multiple goroutine safe way
					}
					cd.flushDeadline = time.Now().Add(cd.flushInterval)
				}
			}
		}
	}
}
